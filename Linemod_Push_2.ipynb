{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "ac7e1f38cfcf42698307edd6dff44a0a",
      "89d61e878ec04dc28a1616129518a0fe",
      "f3dd0fa6a0184765b98bfecafa12f655",
      "a0504d3d8b3e4e94ac803ee6819a3e7b",
      "c73ccb322a7b49e19d8ab12f1fe50667",
      "0942a3da5aa84331972fbfbedc26faa2",
      "0f1314fa65aa4ef196678fea52fb8551",
      "35b7d871bb47440a8dcfc21950003c18",
      "a026320865e04f5a9208cd9160f1658f",
      "4cad60bbe6ca4986b3b8ec7abebcade8",
      "5dbea5b1f93c4a0484ef14d83e122cfb",
      "e63492a34fb84f2bad0b169b41c2aab2",
      "74117c8bfa0147a7978ec895fe87ebc4",
      "8e46bd20bed642ce9613689b6edf15d2",
      "b6a4e41ec8c2451dbd94c86a4c0f8ba0",
      "72c72ab50c32487783068e9eb7f1395d",
      "3de6c69d52374ae7bfd9ecd63606c708",
      "6390822faed6466cb72354f83f675761",
      "d7265c8fc1c142e7bb340f2fee9f4cea",
      "3c2dcd857fc442dba58b038e9ebe780b",
      "d46965ef65f5466f8a47232600b32e05",
      "0b4c78cb65ca439197d1c033bea356c6",
      "170c995283ca43e6aa974b0200312558",
      "121aaf98a9234aebbaf12d7e0a77f284",
      "f04b1f0e46a147e5b80fe8f320984dd4",
      "05dfa0af9878486496eed0b344802613",
      "e585c4ce4ef54762bc56ef5e8f74950a",
      "bcaab5f12c854ab6a377349284df0bd3",
      "e7fb6fb9c3ca4cc7954937da3ba407ca",
      "5423882bf32a4c3aa6915dc597517ed8",
      "438fc423c6fa4aae8b32e52736af6ccd",
      "e660189f24874c979d838a20a540c726",
      "e1a7e2b3a8f04c23a0b78b8bf86b3a8a",
      "7802b7ed17494d50bc97ebc6f2f1d0fc",
      "49ba2f228d874da68e4ec60d52f7d3cf",
      "98775467f73540c9a0c2012b370c336e",
      "7003fb4badf44b00b0d48fbd435484f1",
      "904ecf8997db4fd7b23f4120e6c68f43",
      "2615861480654268b94238438f7bd0b8",
      "0f772adfaae842218b881251c0333994",
      "258cb4dba37643138b02c9b1c42aa898",
      "4225da91aee946b08e6c63f0d60066ed",
      "8ea4f0a0ee22437a89a7ff650c79d96e",
      "1afd098de2e0406a9682fe23918db6d1",
      "dbbaabf256f44f4194b73efe0892308d",
      "37d880c7dfc44a43991a64b8f7ee99ba",
      "ec9167e3956245ebb1706e6f747463c0",
      "f466ea337f084749800fe9da7c9cf4eb",
      "96b72a9efa4341ce9e637a2427732c3e",
      "3092fe0caef749ebb83af89135e836a0",
      "023d0b06d2dd40dc8971c88aaf74ce8a",
      "6fa44d91ab2f46fcabe98218372a515a",
      "8dd0bbbeb8f1425e91072a18afaf8875",
      "ff29b4c96df44e2badb790d4b8cade9b",
      "b57d3d585cc644f7a37697280ba71de3",
      "ba5a92b7b09047a285232990c29a0385",
      "f681e6a4fd8e44da8d8494df0ce9f924",
      "826e73062bb247d89fbce65ca5cdf73d",
      "cb1af0df88094ce98b02e242767eef50",
      "a7b2414444a6472f9fe48bba3e0e3e37",
      "6bf57547b4d34c319aea9bb0d9a0b55c",
      "619e0c0591a74bec8456009ab8a22617",
      "09bfb8ec940e4c448148662440140b87",
      "cd158ab9f65e4b818d2c5d39607abde0",
      "34597d90880c4c7990a0d3e2d46b8835",
      "6be03dd7afc0495ba22c8bf5f501f12d",
      "9d8bcabc08384b41a4cf3b86971c6501",
      "47ff2d31721f474cac1fe21b3abddc3f",
      "3170f89f84364529bde5234fd5c8ae12",
      "3311bddb34ff43db801e606e85f3b117",
      "dbbe840b7f7d45d8ba26930f5397adde",
      "75e07cb640e94e0b9846f7e9ee693381",
      "e08894da19c145719501ec1aa9740f54",
      "8ebdb5fcc8be41138ca6d426bb01a607",
      "ee07967026ee4ccba846785405b0aa30",
      "42e33a76d567491aa8b70e74962f7934",
      "8fa3690a3d2644ed8ed3facf7e141b34",
      "911485cf7cfc474aa65524c6eb00f3a0",
      "2dd1921608124b7590e72bd985765878",
      "6f45b5bb6df844eea47a7d2014051b75",
      "62292425c3e14d9682f057215fba3ba7",
      "49eeabd618dc4588b51241db800dd04c",
      "618e0a700d724f33a7f2e0852a194be1",
      "8fb643c7ae5e4bd9a2807b8824b7d2b9",
      "d6e5ea8238f84a04b6dfddc34fb1b676",
      "b1d103f694e6499b898c3af209982da1",
      "c2c8a0032ecd4d9f91e880d93de3b56f",
      "772db222208f4a49a0859dc41e49f9a2",
      "c0bb74bcc8a84560ab1e6491be9f515c",
      "f4254324e11a4adfb9b2965c68c75b9c",
      "ffdcedeb72244499a958002c9e0499bd",
      "0c6e864530ad4028b80f209ac31fef06",
      "1d26fc1e5a7449fcacdad165d97751b5",
      "31d15ee68f6d4e5b9db0518259ea315c",
      "de79026bcdc942e8ac1591108258ace4",
      "1da944aac5ed4f1a87f9a11319e5e6a9",
      "eec34da483b84ee1b2aa3fa560f84918",
      "51cecd8bbe424a47a31d4a5f456f5104",
      "e1c249b7386b4c248af359ddaadfcdcd",
      "3594f36083514724ae6ae76995c946c9",
      "0662451a5742479a9ff4a5db2a8d280c",
      "ca321af3c0924690a3d834c493a4f112",
      "2a2b7376f9da4041a0a45b293af5114c",
      "2311bb704bf8451c8d3883cbc5349cd7",
      "1316ce231240449c8e3b994c5257ddc6",
      "644ab33857c84fccb22aa46022492c49",
      "8aaeb3a00dba4a428b29c808930d8c01",
      "00106e98362d4d14bbfc96208012a724",
      "796e32e89b2b412c83b59555f2f28601",
      "478f4d5f3dcd426b846819a18dc2e78f",
      "5f82fd8e83814c578072dbdbdf1078db",
      "cc690059ff134f8088179a68060d2390",
      "30bbe48bc2ac44d78758ac44b237036e",
      "a96535dacc1749f593b317c1ff10b021",
      "332d9acbc53443e28d79e36e5c58d1e0",
      "dd324e32b1d64859a31b589c272d4eff",
      "3f0fcd235c5c4ac0bbab7813241603e4",
      "1a80c01102cf4db88fb40253b3ce2a4c",
      "bff56e4d585b4d71923aeea66666259a",
      "8526642f415647a3bca4ae1fc58337bc",
      "7f966393bf724d08bb8cbb19d42e59fd",
      "314c688977b847f294fbcbd175dd4e58",
      "f09931a6c36849e7a6c1439304a7b5be",
      "fc23c5393b714457a7c4cdb5208bf98e",
      "b1948f95644a4838bde291f4a7249d31",
      "bf561a487be74949bd449e5ae390a64b",
      "2326de1bb4ba4cb5b1b230e94cb6b89d",
      "3cafafbb477c467587e119ae9d916e55",
      "b6e115618c754eddb6ec513659906921",
      "c937103cb2e94df6990aea8178b5e1ba",
      "83b54d87ad25472f8ffd34bf9c236a5e",
      "fe990fb32c53458bac8d24cca76e5808",
      "6f9c4efc82364e359b33eed1e4575526",
      "cbdc77b024d541bc9de0d5f9c7cf6cb3",
      "a6951192833f47179d06693572d7e4f3",
      "6a119054a32542ca8cba980336d0a2a6",
      "18081837354548789fb75e0115e4fef6",
      "136bd8f4e8c04a108b911f710f3cfb12",
      "20e033f2a9c64e5d8a0dde6d0589f107",
      "00ebb04bd5bb4c27a26fab4f517bd560",
      "e616e1c78dc7461cb3e6110c373dbad8",
      "7b0a85cd86df494f9cff30e7b28568ca",
      "9259fc712b654837a96833c9d014dcd0",
      "8c7062f494ee40b8a5643b3131497343",
      "8ab31df3b0fb4dd28b001670402cf1b8",
      "095b803945f442289e3c9ef809c0ca1c",
      "0af58fe11b7a433baf181f6cbe490d61",
      "16ae21fb21824dd5ba74d3b33e8d7cd7",
      "79c7b8b41726410fb1ee0688e522a7b9",
      "f956dd78e5724d2dabed2bd62f90e0d3",
      "7b4a726587364561a850e37d0f28b026",
      "fdc6583232f1426498dde6ea65cbdf4b",
      "a12c0bb2fd234e1586e73410f3213c11",
      "07c976d065874b318a2fa1e63b9af966",
      "2a8d10da3d2044cfbe08555f4755400c",
      "e0124977b9e54603873389d7576dfa1e",
      "30db22ad303348c0b606bd3c13392360",
      "b2080e7917364612ac0a85e075aaf7da",
      "088ccc84780a4989987598cc2b3fddc8",
      "3a28c916ea0e40318a019439cfd463dc",
      "501f7787669a485eba40521fa5ebab23",
      "a00215c5d9d348dc894f5114e904c2f8",
      "c8eca9853a014c4fa711509513f13636",
      "e0143b3e4ebf4711a00dd4f7d8906572",
      "d809e85374604df59d62049c2a24bbaf",
      "d5ca0f3a517549528705960036541148",
      "614f9d3c61fe4e64bfd6dcfbdb389167",
      "735ddec7c35e4b24af519ac683e7fa10",
      "78cfbaebb7dc40b9978739997854dab3",
      "cfe4c850258e4766bd937f001ca853d3",
      "aaa56d00430a47ce85eea942da84878e",
      "f506ea09f69a4044aeacfc30c76739d2",
      "2ae8460949984d69a94d99b13cb583ff",
      "ed926b19b7534e189a80fd5a3f2de44c",
      "bd20fb66082f4c6e838bead0b0f12e3b",
      "df56d432b7cf4fe795248cc6bd58499e",
      "d5aa76dda0ca4e0099008646ea9625e6",
      "1698b804e905449181552b1f7cb3c455",
      "176c6e3c64c44e1dae123fcebe953ebf",
      "5ead565a09f6417991c03ab4a106e34b",
      "4e0f0f5e89f4446793940a62ec8b158f",
      "60ca0e53bc7b40bcac65c66af1eaa95b",
      "eac9a4d411284b398fb9af43989c22b6",
      "536bca70431442788407bb7acda31bda",
      "c9fca356e60c42a2b2a3f06ab57fbe3a",
      "d0209d6ceb3744ed849080c284c2831b",
      "d4c1bfa702434cc6ad78ea99b939ec36",
      "233dd4f1232242e68eaf6be49f9f43c1",
      "a8b8bb461fc046188e6f4e80ecbae9c3",
      "43cc9669fb99474fb7488a2b9507168e",
      "db646bc6cfba49a7ba203146c303e059",
      "34fc84142d9f4a2ea95d2db2149c90a6",
      "f0f8d1f095434534b4b6d5b9d006b9dc",
      "67858623a58349feb9d5dea36d3b5a73",
      "250f0781a52d40918dd503f0d5282200",
      "90f491e3d05c4e4299c124763457391b",
      "f2eb057f78b94af38b00939cbfcfa71c",
      "e8fa6e56b86d4e318a008630ce895bd2",
      "42d56d0a717844b98da1b7f132175878",
      "555f3e306a204fd488e4837e87820199",
      "f0e30e93441c41f1aa3b410ec03bc142",
      "fa366c3907e14d839891e1435e466fad",
      "f363f205d808446fb3d693cb93a1e37b",
      "8a09dfa6109e4c7fb278a12a0c5d7551",
      "476a0c7d06994d4ea371abefddeabde8",
      "d9a958ed481b4e3ea3e01a8b33336e34",
      "24a9e3fd587c41c1a903dc7d4e6638fe",
      "3c102f1eabfa4468ac1e58ee1b66c003",
      "09424cf2cb0041cc94e80c60ab6432e0",
      "69e47095bb104a158032db48835b262a",
      "65756bc5eb79456b9c65ff094ade0707",
      "4bd5e713b88f40608ee0249d60a271e3",
      "b394f0fbc65b4812a15a45ffc20165d7",
      "fae599eaa8b94c27bf362c506d1b3c97",
      "67d6682ef6ba49b1a238301dde1e25f9",
      "3ed1b8784be14961b6438d6e6f6ab0ee",
      "43c564c90427454ba1adb6567b3f3e14",
      "4dac27c1fa074ab4a53841864b932bf9",
      "19c366e347014e07aa1cf4175b886817",
      "10895265e66542178a576851817fdc3d",
      "6deaa1de04174fa1a86f275d4ab3edb1",
      "55484b17feb04896bcbe03b7c798fba6",
      "ce80089a2e83422ba1da86120f87cfe2",
      "82c2955f094c421d8ecb9eb12d0d7706",
      "b5be3145166347dd86e6953f0b7c7335",
      "12d6752845954fcc95aea6b9a65ce312",
      "c769a9d1d1d4478e865c41a179b4c5f5",
      "56a2774a870745fca8e220f4dffe5a1e",
      "ba4d1f61e6c34e9f93419b95d1632502",
      "8f2e8ff015f94836af9a901d0b026c66",
      "4849a120e7434264a160e57520e62147",
      "65ca6351e1c64fbd9309bc9180eabbb5",
      "b4ae0f194b4549988a0b928e5d20b63a",
      "517dc4fa44fc4bd7bcfdc28d5d64ad36",
      "febc55dc989a4a76b81393d02eb144e1",
      "be1e41bbac5f4981b1470713b2894669",
      "ca94fadd36604642918fa54eca833c6a",
      "3bad278ce0604764b0e63b40d7e5dbb0",
      "d66ccf0c111b472d84d9b4e44a0261a6",
      "c8c9baab4bf845bf80e869ee0e9a4692",
      "37c766a15ac64ade9569db5722b21e1a",
      "a8ecab4d7cef49f18226698d78897c44",
      "f4543d39ffee4ec5ab045087f98a9385",
      "cf5e4b60dd0d4f9fac0fd3377e4a4a1e",
      "e6ef8076f6d548bf9d7763a1e290a971",
      "952d098034f841b1b62c5f6f93e3567e",
      "fd5215c7a0ea4f4b9e7c3c7937c853b0",
      "760dc12d2bae4700a96df85285a7178a",
      "21d631e78ca0468c9839b979df8c1978",
      "c4fd14821c814bd1944fdef6513dca41",
      "4019b3c9579d4386ac26f3a1bd5dd955",
      "5e88a225a5dc4bcfb5cc779c2759fe7c",
      "7690b4f33cd241b98259565cfaef1a95",
      "75df80ef7ca445eca9aa839a5c890aac",
      "7bb90b614884415ebea0e3005be52be8",
      "3db6b44915404feea5a761bdf33670eb",
      "d8e572a1f79849d182a793bd18fb43f4",
      "fe8fbcd6dd1349688523982a0e6a8a4b",
      "5946db8d40344f079c45b8eb5b2c94bc",
      "0ae0d3bc70b8416a8a3ab741c37403af",
      "0144b09e1bee47c882f75dd656a81f7b",
      "9fe740716fdd4123a2eb9bec2c0c279c",
      "f920f78d82104cb1b8d4130e539a31cd",
      "2a34b4889fd04e829d69400cac0b65d7",
      "907b9abcc13b455caa5d9782bab0afaf",
      "e32703d8e35b41f3a58a1e4144ec9a08",
      "8af7031a7600446b960514bc9724259f",
      "eb44a9fa546148beabe0fdae5f4f3271",
      "4adce52738fb403b8a97142486c6508a",
      "d808e18fd6b8446c8fd46c3d57fd5891",
      "d75eb91f509c459f8c92746406d9bd52",
      "5a1a0b13696f498cbbf4900697c502cd",
      "fc277ccfe5c7429e9266d520eaed593f",
      "9745873d64bc462bbd460bcffc742980",
      "dcc6743bfc1f4c028eb0b30d5ac3333c",
      "02e053910c284311bcdee46ef3207974",
      "b9a2ff9db1d944369d3958da901ac30d",
      "b8bf23d414644643b9df7114c38e3c05",
      "15f6ef62c8cc43198e6938aa7228d4b4",
      "5dda810b79a74b73b90310b5ff1536cb",
      "d33625ae0c0442ebb889d6d82d848244",
      "ec810e459e97455cb7bb4c95ab1884e7",
      "90692f61264b45058c986b022407281c",
      "f147c4796d3f411eaf8e606a6c581d80",
      "da013dfe1c34432486cbc5ac94c63e7d",
      "359793efd6e34d03ac2c90eb12b23558",
      "972bc49628624b15980af9d084c1810c",
      "43da3797e4e54683b9c217b54ed2df6e",
      "2000947a1660488c9d932e58b28448db",
      "a07e2845dbc44914bdb64698bc95fd2a",
      "1efdaf3a97d04c1fa456ed5dec9b33e1",
      "42d3007b38ab4728ae67e3f2f867164d",
      "daab8fc47c5e48b9bc89300b06c9f45e",
      "d4094d6d374a444296a74eb8e174fe36",
      "1d3b0953a7c84afa94b67759ba2779f8",
      "2941b91ae21244518cbf020f7e162743",
      "4831b6316ec844529953c4c02adc66f2",
      "c2439d9053a545e68d4769e3f6e37afc",
      "39339f76a0414509ad046cd5d94179ed",
      "125d05fcbd8a43648e0e4533a84ff83b",
      "cf26cd8bbc9543e2a559d57e9499b914",
      "de125293645249cc9e3a67e208a5e785",
      "d17fc06821c54ceb8c8aece5f0ee6d30",
      "4073a70826504e08a16802cead0a4972",
      "0c79442ddeba431ab32982d97f44a985",
      "7d692d41256948d38447405de835bbee",
      "347407c019b34e25bd77a7d8f1948a01",
      "2e5719ab09124ab6ad1e4b8fcad4c6bd",
      "b528601df7d3493f9102f964e822b7b2",
      "ebea885222ed476894d5ef7310b3496b",
      "b14e0b9a16574c6a9b5484e1cc5d5128",
      "b562b4fcefd74dd1936eb375be3bfccc",
      "79963b5987464c66a94939a9f9ecedd8",
      "378420f2aecf47e18a62ffb0bbfd2066",
      "a10fd18d79834487847875f55b45ac2f",
      "13442bf83e6f4f4087c179ade609b318",
      "6e6d5856bea440679fb12f009e97e43d",
      "a2807e9357164293b31765f42535ad46",
      "c913aa9f4e6b4e6ea4c67d793973e438",
      "46010acba8214affbaf81b3204cab5a6",
      "8b277961176248ac8e953d237a137600",
      "53460267bca04b94a4c14d5ec1dd0d1b",
      "7fa0e918e11343ea8ef59a1458812235",
      "88e110e8b2a9447aa2e24e3baf671f82",
      "c2871b25daaf4ff7942a7942f4273ca5",
      "2777aab49cf64f60b9162a2afa95ec7d",
      "433e32abada74dcca13f732ebcc4f87a",
      "a7f78ea924ab446ba48dce8e64ddf9df",
      "a743b9ae8c1f4a43b76e59f3ba05e4d2",
      "722738af2e3045ef85c2416cdc0d6a2a",
      "29d5d9a1c2af465aad12701d538c8d13",
      "7e13aefb44ae4803810e3e75bb65b2f2",
      "0aced6a0e6ea4f6181210481acebdb3a",
      "6a57b98708954530bb0c38f11fedddfc",
      "c0c1e86e9a814696afac7f5306a9d434",
      "141eadf874ce4da58cbaee4112211d9e",
      "5955a0ebf78244ae9cb350db019de145",
      "c3d6d2ceb2d849afa910f929cc66d5a4",
      "8773138405834461a32228c310af764a",
      "f72bbc52cd674d5eb4b65dad6a958a64",
      "1508970211d04e619af154fb4d1625cd",
      "54c3ec8d221542cba7274ca7ba369f71",
      "ae3543faa59b4ad6b8f768f47edde40e",
      "7268f06e2c464b01a10026be34f4294e",
      "a4e1f98608d846fe8c65a9c495402c31",
      "4cb2d7d545e540889a8cc6c51c95b8d0",
      "341ca2e106814dec8d579fb6f5059014",
      "38f1dabda1d14fa286b63d32ad7f3ef2",
      "d4e8ff0ca5cc418a8d307425abb9baff",
      "946e0d99d17f4b4995b26d74095933be",
      "de5a8bfc9c5341dfa35a1f67811dc5a1",
      "13bfb94ae7844d90a629ecbc4b75da6e",
      "1c9159207e664a198f95677c21f44044",
      "30352adb18de4307ba1adf5d2d19314d",
      "97377e10f2cd4d65a1d0cc48f10005e7",
      "51d9e127cfa945e89be43d0de02b8561",
      "c05a35c4f31e4b86800afe6e1799e951",
      "130a852a52394192ba35059c5678225a",
      "4d1e21f1306d4c4084401cd1d492715c",
      "7625ec159b1b4ceab47df643c2c4bb6c",
      "b356674bffb34adbafe4c2e505165102",
      "de66e9683c324d4383b853b68e6a99f2",
      "97269510ac25481a886359e6ac2eb89d",
      "7a8d01c21c4b4169bbd2d43128634053",
      "b77a5de5f927425496b19585d81df4d2",
      "2bd63a9cc33748e8ba1653fbc6d5f31f",
      "fbea8b77b20340f09626ca3cbbb41f04",
      "39cff3aaab60426490793680d4626402",
      "64d3b217fc664a659d833b6a9ca60f30",
      "eb516aab03254a3caa0581da8cfa2e34",
      "175c94ee36b54abe8269287f05fbab3d",
      "8911bc2fa8734cbe9bdb599f2477747d",
      "ed07e3e8c1474ae8aaac08d5af6de68d",
      "c1375f964f3c4965ae94cd389d1e837f",
      "57bbbdc7e67848fa9d25700c8c650557",
      "1648d6df0c9a45de96d5faed076d2068",
      "d8eebbf446e04386925fdaf1b539bfc4",
      "1b50ea7e4e424dcc9130d4b04b6088ba",
      "8c92e4df01124ef08e2b484e8108fc20",
      "cf32193f9cfa482fb8ad8e232b1a214f",
      "afc9bb11eb4e44828adc257a97deee34",
      "0d1d21bc01e040b5a9ab9af7feb41f57",
      "226f74069f2149a6a61608290855e8d6",
      "477ff25e72654f6186b39da704b06be5",
      "ed4362488a474cd3a17e02d1aa83ce58",
      "86bdb36524c641d1b446c20879333405",
      "1551570050ed4f27a07778dbb5b60c42",
      "957d1e1f03104163ac8270869a8b0455",
      "fa14d5768b6144ed9a8822639665f3a5",
      "de35cd179d4744d6b1f7e8e9aefb84a7",
      "b2e77ee0ecbd4b7c902ff24e34d1488f",
      "4557f912816a40b6937abe9f1bd096f6",
      "a13b8a5a78d643f2a4a42790adf6093c",
      "def9f3b1bf694ce6a99e2ee48e094c5a",
      "84191e9042e84fac84440507f3084dd1",
      "119e7e660600411d8357edb23a4ce7e2",
      "5281d29be3914016933965a2e7f3ef9d",
      "c62eb714ccc14488813c6f9009077ec1",
      "5e778a04435747e785a25ca8e726680e",
      "96d2b0456513427c8a7e7ce513a862e1",
      "af05eb9352f543539bbc21edd85ec4c3",
      "70072e2526e247ddb16aad60c007870f",
      "dc2f513261ab470fa137e27661213486",
      "016c9d41e1fa492aa45bed7597cc6be2",
      "74393a15b990425ca36b05c9102c6d9e",
      "72e83890890549e0b8a06db03bafcd43",
      "8e9f484d23984d8f8c3ca45e35f0eb94",
      "66b6cc1e4b904d2a9667c769146c1093",
      "6e8470039f794a169677fcb8803301b9",
      "8fdcd2fb237c4f94872d519c3bde6978",
      "afc39b576ab5435f9f0ee0355a54b84b",
      "4694934f7b234cabb81470273c9a236d",
      "aa12e4317a754db4bb70dcbf9cf2e1de",
      "cb98ac9c80dd45aba79a3561045bd80f",
      "c5d73f4a6387422abf6fb39ea19df374",
      "b62dd91deb264274b3f52851e9d6633b",
      "9fdde4b6082d4d369d54f8086f5b3e9d",
      "31e702d9f27d45558740f2e6175c516b",
      "1823b8353f0545768454a7c0493f5c76",
      "0e7a4b87eb344ef8b3a996a82ac29577",
      "390985f070e74bb59d04778ac67755e8",
      "bb4508a81be245528d00d187b2c3a124",
      "5807e2c5c9e74719bf301f2001fb394c",
      "0911fa131031445ca0d52dd2b06c9f91",
      "fc816f1397ef4c93ab818ea76441dab5",
      "f544555b3285408c94c1c80439477129",
      "17707b27a9964a139b7d5dd7ceac1480",
      "e3fd21e23d8b42e9a0a531ffc01cd040",
      "625d41c3ddd540c09fef06674140517a",
      "c6b6179aa58148cab386f92af4ba1ed2",
      "50180d77e34649408c6b4e6323a64c79",
      "743a5ce4198b4db495c606dccad2184b",
      "ea32030251d545509322c9415b8c43ab",
      "60bae3da7a9f401aa3da458bf09f14d3",
      "8808a38fb97b451095a1552d019d7811",
      "b5ebb0749bea4751b5501e58e8deb026",
      "d0b24bc2422840398ce36f58d2ac52a6",
      "eecc0885f46d45db8c3364bba6eefc24",
      "eeba24f5382744ca8399d7b17261a0ca",
      "0faeb7b568214731b5f48f4935b4a449",
      "077713e2f5eb4d748823f16ddf363062",
      "4dd62bb88a5d4b92aa30cd24873e0e79",
      "1d20d942f71a427683f2bab7f4c274ee",
      "6413fc3a0a494e2a8589606e4b445669",
      "05c7fe43fde04ced928cffb937274be3",
      "3e8376f56783492a97bec2ed90b970a0",
      "f7a186d7015349beb143ec5cb982e53c",
      "d4a41ddca1194e6297aeb9de73831d5a",
      "6bd580923be64b3696a74d52a5ceb0d5",
      "50e4a348c28c44fab0f5fd8170a1ea70",
      "c70378c6ecb240e290e8f5e91398d83d",
      "b5f1f38fe6a941c3af4123cdd4a873f1",
      "85036c8a3b6a4a5aa74a38b6f8228bd0",
      "85fdf432d10d433e9db3fedeb8e4d350",
      "0bd79ea15a424c97806523a1708a35af",
      "b9830bf876b04be5990169e29901e3b0",
      "eb937effe5994d2a90e25c68de1e4972",
      "dbb5cec2422649e095706e1d6fb7ca4e",
      "103d3918576648a4a05c495fb0aa7d4f",
      "5bde5cfffb2645e0a3283307608bac72",
      "406afc8e62574d3880fdcce435ba2cf2",
      "5dcf9f7416b5485c8394e47e1bcbb907",
      "3c222a712f8d4ffab0f470a3c694d980",
      "e75ea867f9ca4b9381ad4bb54758016d",
      "06031b33beaf40e6abeaec6099d62220",
      "3ff7622f37ff4d679d9e0e6f09d20453",
      "bc70d9c2de89442b88a929988b58fd5a",
      "7d8dbe97cc7040fd9ff1a9c817ee229b",
      "4cbcdcdfd041476ab5080773ad6ba6d0",
      "4f7d93e4ba994b2dbdbcfbb52c9db626",
      "54c7622dbf1c45c294afe67912c94600",
      "50d4c55759944b0ab7751e2fc16bc600",
      "c3f23e60aabf4fc0a59a2547eb867cf9",
      "a2283ae5695d4b4bbda85d1fd3abc988",
      "885298e1a52244809e4b0df12ac65686",
      "a4fcf2a8951b4ae6a8347948ac9b2703",
      "80845711aafb402dab9ef11be076af47",
      "9f0890ee951f4eae84903d658aab1a4b",
      "5f6db895c303431c902b882a8fb6ea01",
      "9f2ac85903494527ba1e8413728a190a",
      "101f57cd104c42ae8b1f1b6cbcc902ea",
      "06b8e586d0b5456cbdc4c24ec35a7f9a",
      "130a20a09df34b1eb7a64c6c7bb7bd21",
      "a5210c146a464d4da0a6fdaea285ac70",
      "30c8c815669649a2a7bbb09f7b59f874",
      "fd84a00ebdd745af817d3debb30b3de2",
      "561ef801dee842719636bb884f0bc8c4",
      "7dc3ef276fea4464862150942208846c",
      "13cee5aa626d4b09a25b89dc3371ea44",
      "815fb42b29f74e48a289560846e697fd",
      "7f546f6243af46d4b3f5038d5aad315e",
      "afb11d64634d418485eee5de5b1ac354",
      "3ece2b25c69a475d8bdc14db5c144902",
      "a45bfba96e6e4e0c8d09eb86813661c4",
      "03d8003166c646c59d4968c70e043fc6",
      "c1c532a383dd431596bb642b762becd9",
      "d8cf0e9c90d64d34b45f445162506068",
      "0c9c912878b5422db85c3c4aef544d3f",
      "4e8c7c6f870e4f258f8faf3c6ea0d11c",
      "012f24bf16494469bfac44eaf578aaf6",
      "c00e57c247834a1c92db778b95f6636b",
      "9f0ab33c5bfe4e28a599c3e065eaad04",
      "ab1afb50e357468cbdb4594262602d63",
      "6377ed79a33249b78fc5e4a6817656f6",
      "26b297f0f3044b44a8e49ca9bd87069a",
      "7a4c125a046e42eeb5769b6166f04fee",
      "85baa18ca3c44304b4a38785867249d4",
      "40be0b60485c45b68082003842e6b15f",
      "304f77a9ee5d42f1b8575d0c254f3c2a",
      "a3c0ba2307c6477daee3a8d2c8d0663e",
      "169d850db6ff465a810681a1f17fa2c0",
      "29c2ecf3b1bd4f3b872d88ca88d1ddf4",
      "7c2b29ebb48542768504de8f1b0561c9",
      "c8482ad4661b4d22963d9be796dc98e2",
      "c38a4064596e480faef94ae3e893a738",
      "a7c0c983dbe6424e9410269850f0a727",
      "45e7498b6a4e4988a27f78e5884a1271",
      "5ee86c66336748c5b33dacd8adf86661",
      "4a540e7c70344bf0aa1627b6709489c9",
      "8f7922d1e51344ef8977356869e6115a",
      "95f5ad1a0a9343f19827b7edbbc3faaa",
      "76123e3057834ded84c5e2e83fd0e4c3",
      "315add74134c47289bc6e9e1617973ae",
      "8e1bfd06b3aa45e4a082854b968e0130",
      "f697d3afbd8646ee8f9782aa24bbe0c6",
      "2bffbc42e43148859c2b3f92df1279e2",
      "21a998ff3ef7455c9452cffeed3c779b",
      "9fb0ea9d4fad4d90bb38928ffb7ab764",
      "53277f7ba2a04018bd25ba3d8ae4e101",
      "40431e17250c4622897fba23eacce026",
      "68e6c1b115cc42078531725bc91109e3",
      "f8805cf4fc5a44cd99b804a1b6bc69f3",
      "542854e2af34406ba67e312d6e34d2fc",
      "488d2f7f4f3d4a42ac6f0cf20b96c44e",
      "38a1f0b86efe4a90a4eac7f70bb4d97c",
      "b65f478a1d664dc8895c5545fbe29924",
      "7eafafe68ef14257abb7a14b34c6b59b",
      "546b3f6b73bc49069a201d08cd904f78",
      "e7d4e5b584704d1ab15c8c5c001869ed",
      "6cb0271705a44cefb00a92dadce3b0f4",
      "7859594b178c453c92ca6f257e9d4e28",
      "18fcb30bf3684ca195613451da80a742",
      "22d3ba1cc81944b7bc3b466486c4171a",
      "4bab38272530463cad6f2a1a0117231c",
      "1efc4e5c13c5499f8e53d265c19ac206",
      "12b3649e1e084eedb4df4f16f3e448f4",
      "3b8d1e810ed04e5bb82ebab71d69a8a4",
      "ffca480fda4441d2b788d9806edf2a87",
      "aa19ee1f645347c4855c0cd80c4ce66f",
      "96d0b362ac2f4e028b5354f64149a079",
      "8fe31a953d054d6d91b8b8f9c9ee1025",
      "43f785d275004dfc93edf85692dd1670",
      "e5df0176da6648b2b32a6f9a40e0e1f6",
      "5add3994e63f422a818831de6c16484e",
      "36a024ef95354f36bd4cee1babbbd04f",
      "12e2efddec08483cac185c552bd033d6",
      "928d394cc7814b5ebeb6ed5f8678ac8c",
      "31d5c42e0e81406d8e006b842afdeb57",
      "591ca613b1f84d6d9d1e20c78378590c",
      "33c555a45b294f1e95d523d253218fd6",
      "4243e590ff344dc29cc87f98c51b7fad",
      "e39409a4a9ec4ccd8a9bee9fa7b474d4",
      "eac48a7e6ff6498495ebb18d38ac268c",
      "963abca5547146328b4b9e123cc8318b",
      "d92d9346ceed441d989144cd06f24216",
      "3140f3bde914455eb2da09df8675f9fb",
      "7ca3e656a3724b86b480adbecfd4132c",
      "25832676baa54964937275eedad7de8e",
      "9d1d37d663ca4f7badac6224cd03d2e1",
      "3528c1144640419a9f4726142ed60088",
      "a94fe7ebf50a40c2bd9513a1929d9230",
      "8e8aa2f972eb41a5be44ec056494d4bf",
      "45b9a90d9a894207b4d3a5d0deb64613",
      "69624c8562994568bf9a76272bc52f3b",
      "0a3133c0dbad4a1bbec46872bb7f13ba",
      "0d1bd45241e24a3dbfdc1f53884c2ca6",
      "b8517c2aabed4114af5ad7e49bffe7eb",
      "fcd65fb1b14749e6913555d4de31f863",
      "cd402a5f425f42b39a0b7960ab1afb2b",
      "dee6b3eb8ddb4fc8beed0900cb601590",
      "9b658c7af0284d958a7b8e24922ac191",
      "665754bc7530474f9544f0df31724e19",
      "a50b51a66cf840f0884ba9a92a75c116",
      "dee1fe74b07843588c1f697b2ad013f7",
      "2b9f6b541087462cb068d41c151d7514",
      "d609d3b4617a4bb8b4be204e651b2116",
      "065ab27d6f7d4caa956005860bb87a28",
      "caa5f6513c644b24b528bbd3e34197b8",
      "cdcfe0435a7445fc8a373da77ca77797",
      "aea66bbdd6214e6b9b00cad7adc1ab3e",
      "34b4cb2ae66d4845b6c8d985eb73bdf4",
      "a1ed8c7227a743e79b399240103a268e",
      "5d564e3ba1424b30bd59f1720fab4ff4",
      "b7afd1f93f1b4d3f91a3adcf3d200ad4",
      "383ae66fb42a48bc978fab11b8790646",
      "ebe943bb34004e87906406245b4eed3e",
      "544abd478a064ce297c9246cccc9d531",
      "62fec1b2f85e4cd280562a825c6769c0",
      "401be3d78b864f72a329ecab119055d3",
      "85892316f475469bb6a2a8c35839b110",
      "54cfc63b195c40c69cec15d1a4290bb5",
      "fe028153052541688b843f5d2bf51ec8",
      "a37cea12ba984bc7b07f7c2ae80436b4",
      "5e3065cd69484a3998385445758416d6",
      "9321b68f398141bf8ac476efea9caee2",
      "ae038bd82f4247b2b5af9a76298160b5",
      "ef56be7684b1499c83a8c23b3e679d7c",
      "04b22f5538e54f81919e58fc38f38103",
      "042da6a7d9b449a0a1fef83bf2d3b915",
      "24cc2ede98f04434a8a3719476d49f5b",
      "b0630091d05b43758beb1aca1547876d",
      "ca95b7c055064a168667a8414d589542",
      "6d1f886279d04d599d535fb852072da7",
      "4c370783bb5d460bbaa512c30769e5f7",
      "247fd5dff1114c40b6d5ff4dc82bbf7f",
      "73a24186da95485b88bbf9cd6e8c1ac2",
      "3a2c7a1c4ca1474fa7fe4460aa789fc4",
      "ac183dc4d00b4bb582c22f4bbf597ed0",
      "e3f9fc8c5a2b49d498f832a5afc3c1e8",
      "13525ac20632479f97893bd3a667c3f0",
      "b2ad97be1eb949cf8715101ac5b25f2f",
      "9a2956356be541748b9e748db75c0262",
      "0c85228582d2484b87514e5158bb51ae",
      "9d1f7ec4035b4ad9b27ee3cbe9f20cf3",
      "739e0717301e4e99ae83eb152c4238a1",
      "6fcd8d77111b43289c89241ea5cf7426",
      "40257553b4734151a05244b7da2fb828",
      "1765b55987e745d7aad1f248ced2d6fb",
      "92a1f6cb589a484ca3e743b776c896ce",
      "eb3405a4f32a4307a1da3730dc80b22c",
      "726d59d92b40432ebf5a9224d91aaf80",
      "de9d068cd4f1489aa1b7713fdd4f35f9",
      "8178c8f448bc480d82c6d74a9c4b44d0",
      "8f8626e881314eb6b16f96b9b3f5b0c9",
      "7738d44aa0e04e6299795fdc700b7163",
      "8f122ed9a6d945b2b2df7288590ee0da",
      "c981d4f1dfb949cbbde12c0e3d60f148",
      "8f2ff37ea1d14fddb744ce724ff10b78",
      "758a83fccee940f89394f5aff7e79fdc",
      "49bcaf9934944b2b827283856c0f7080",
      "b354700532464a7686aca94a412dcc59",
      "27ff4e640e5e4db0947e4aeb867d8440",
      "b0f939304f4a4fe5a0b73cd308d53e83",
      "5ccc9d6d978d46bbaf587901feae47ba",
      "4cf2711b6d4e4f4db0d786760b85771a",
      "539cb13e7d3849a2bc83733599f5ab49",
      "9a4fbbed32ce40bdb67d54e05870af60",
      "be454a658e804a428c56a7f5d7f5dcb0",
      "f985fc8c0a2e44c7834d4613067b382f",
      "e0d715efe6a54320927861e90c51823b",
      "ce4b12fd356448fd98d6c73a9f883018",
      "0f970e217e46496ca17ed17007e7f47a",
      "5ecc494297064f708d4fbb6931a37502",
      "672e174b170f42f59b9568cd3ce51f90",
      "db5f9806cdb341519981fea577ba5b65",
      "823d3633dc334f9f98bc020720cb0c5b",
      "a49d99856858443db8446e47d9f81f90",
      "3f79d30671234892aa5bfcddc067a835",
      "31b4c5f4afcc4c11bf4560de3c8a5909",
      "0eca9b427e0c48e1bdb2bcf1ab33469c",
      "a43e24034bc8459f850de1d421a0687c",
      "7c4f96c9a13f460c8902e7d534557eba",
      "a487209498874902b8df9102f058a5b9",
      "2f48a5d63a1744a391069a7559982037",
      "b4e5805f5b8d4fd0a068156860bd120f",
      "087ecd59bedc4f978c42261ce4a0f68d",
      "133005bb2b9f4cffa172461cda8118e5",
      "3fcc61b6b5324959ab42e0c72d6cea90",
      "e66882f3281840298605d6200dce765e",
      "225054cf3a0049b98b4314d40b645230",
      "fb2e0c64e9a04407b11a6d60b8d01664",
      "6f80a8c19d9e4f7692d5180931255e2a",
      "834120a377f94e75842e508b0bd8b042",
      "1acd34e8bcec4f478adb669836629097",
      "dd219b74482440fa9e61087871152eed",
      "ae510e9491dc4a2192b8f12acda22f2a",
      "2d1edab469e345cbbeac88d83c925923",
      "25ef92891bb74bbc924ddb93a0722a7d",
      "31d8837362ed470e8e57c5bccf21d6cd",
      "608312bef4b246f9ba6d91b0322bd277",
      "5baa0ed105154ae7989142704b6847d1",
      "f46456cda586479ab4a1a2da951c6017"
     ]
    },
    "executionInfo": {
     "elapsed": 3755962,
     "status": "error",
     "timestamp": 1761317877887,
     "user": {
      "displayName": "Forergun",
      "userId": "12127268639804414309"
     },
     "user_tz": -330
    },
    "id": "VaH5WJUArwOK",
    "outputId": "38f51786-d4af-411f-8a2f-7dc6f0176419"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "🚀 PRACTICAL PAPER IMPLEMENTATION - BALANCED APPROACH\n",
      "Requirement already satisfied: open3d in /usr/local/lib/python3.12/dist-packages (0.19.0)\n",
      "Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (1.0.20)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.12/dist-packages (from open3d) (2.0.2)\n",
      "Requirement already satisfied: dash>=2.6.0 in /usr/local/lib/python3.12/dist-packages (from open3d) (3.2.0)\n",
      "Requirement already satisfied: werkzeug>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from open3d) (3.1.3)\n",
      "Requirement already satisfied: flask>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from open3d) (3.1.2)\n",
      "Requirement already satisfied: nbformat>=5.7.0 in /usr/local/lib/python3.12/dist-packages (from open3d) (5.10.4)\n",
      "Requirement already satisfied: configargparse in /usr/local/lib/python3.12/dist-packages (from open3d) (1.7.1)\n",
      "Requirement already satisfied: ipywidgets>=8.0.4 in /usr/local/lib/python3.12/dist-packages (from open3d) (8.1.7)\n",
      "Requirement already satisfied: addict in /usr/local/lib/python3.12/dist-packages (from open3d) (2.4.0)\n",
      "Requirement already satisfied: pillow>=9.3.0 in /usr/local/lib/python3.12/dist-packages (from open3d) (11.3.0)\n",
      "Requirement already satisfied: matplotlib>=3 in /usr/local/lib/python3.12/dist-packages (from open3d) (3.10.0)\n",
      "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.12/dist-packages (from open3d) (2.2.2)\n",
      "Requirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.12/dist-packages (from open3d) (6.0.3)\n",
      "Requirement already satisfied: scikit-learn>=0.21 in /usr/local/lib/python3.12/dist-packages (from open3d) (1.6.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from open3d) (4.67.1)\n",
      "Requirement already satisfied: pyquaternion in /usr/local/lib/python3.12/dist-packages (from open3d) (0.9.9)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from timm) (2.8.0+cu126)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from timm) (0.23.0+cu126)\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (from timm) (0.35.3)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm) (0.6.2)\n",
      "Requirement already satisfied: plotly>=5.0.0 in /usr/local/lib/python3.12/dist-packages (from dash>=2.6.0->open3d) (5.24.1)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.12/dist-packages (from dash>=2.6.0->open3d) (8.7.0)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.12/dist-packages (from dash>=2.6.0->open3d) (4.15.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from dash>=2.6.0->open3d) (2.32.4)\n",
      "Requirement already satisfied: retrying in /usr/local/lib/python3.12/dist-packages (from dash>=2.6.0->open3d) (1.4.2)\n",
      "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.12/dist-packages (from dash>=2.6.0->open3d) (1.6.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from dash>=2.6.0->open3d) (75.2.0)\n",
      "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from flask>=3.0.0->open3d) (1.9.0)\n",
      "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.12/dist-packages (from flask>=3.0.0->open3d) (8.3.0)\n",
      "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from flask>=3.0.0->open3d) (2.2.0)\n",
      "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.12/dist-packages (from flask>=3.0.0->open3d) (3.1.6)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from flask>=3.0.0->open3d) (3.0.3)\n",
      "Requirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.12/dist-packages (from ipywidgets>=8.0.4->open3d) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.12/dist-packages (from ipywidgets>=8.0.4->open3d) (7.34.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.12/dist-packages (from ipywidgets>=8.0.4->open3d) (5.7.1)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /usr/local/lib/python3.12/dist-packages (from ipywidgets>=8.0.4->open3d) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /usr/local/lib/python3.12/dist-packages (from ipywidgets>=8.0.4->open3d) (3.0.15)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3->open3d) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3->open3d) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3->open3d) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3->open3d) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3->open3d) (25.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3->open3d) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3->open3d) (2.9.0.post0)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.12/dist-packages (from nbformat>=5.7.0->open3d) (2.21.2)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.12/dist-packages (from nbformat>=5.7.0->open3d) (4.25.1)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.12/dist-packages (from nbformat>=5.7.0->open3d) (5.9.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0->open3d) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0->open3d) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21->open3d) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21->open3d) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21->open3d) (3.6.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (2025.3.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (1.1.10)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.4.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.19.2)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (4.4.2)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (3.0.52)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (2.19.2)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.2.0)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (4.9.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (25.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (0.27.1)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.12/dist-packages (from jupyter-core!=5.0.*,>=4.12->nbformat>=5.7.0->open3d) (4.5.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly>=5.0.0->dash>=2.6.0->open3d) (8.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3->open3d) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->timm) (1.3.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata->dash>=2.6.0->open3d) (3.23.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->dash>=2.6.0->open3d) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->dash>=2.6.0->open3d) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->dash>=2.6.0->open3d) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->dash>=2.6.0->open3d) (2025.10.5)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.8.5)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.12/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.2.14)\n",
      "🎯 PRACTICAL PAPER IMPLEMENTATION | Device: cuda\n",
      "\n",
      "🎯 BALANCED PAPER IMPLEMENTATION\n",
      "Object: 01 | Points: 1024 | Batch: 16\n",
      "Epochs: 150 | LR: 0.001\n",
      "✓ Multi-modal features (RGB + Point Cloud)\n",
      "✓ Transformer fusion\n",
      "✓ 6D rotation representation\n",
      "✓ Confidence-based prediction\n",
      "\n",
      "Loading ape (asymmetric)...\n",
      "Found 186 valid samples\n",
      "Loading ape (asymmetric)...\n",
      "Found 1050 valid samples\n",
      "✓ Training: 186 samples\n",
      "✓ Testing: 1050 samples\n",
      "\n",
      "📊 Object Info:\n",
      "  Name: ape\n",
      "  Diameter: 0.102m\n",
      "  Symmetric: False\n",
      "\n",
      "🧪 Testing balanced forward pass...\n",
      "✅ Balanced forward pass successful!\n",
      "   Model parameters: 35,331,402\n",
      "\n",
      "📊 HOW THIS STAYS TRUE TO PAPER:\n",
      "   • Multi-modal features (RGB + Point Cloud) ✓\n",
      "   • Transformer-based fusion ✓\n",
      "   • 6D rotation representation ✓\n",
      "   • Confidence-based prediction ✓\n",
      "   • Same feature dimensions (256) ✓\n",
      "   • Same transformer heads (8) ✓\n",
      "\n",
      "📊 PRACTICAL IMPROVEMENTS:\n",
      "   • Proven backbones (ResNet50 + PointNet-style)\n",
      "   • Global feature aggregation (more stable)\n",
      "   • Better training stability\n",
      "   • Expected accuracy: 50-80%\n",
      "\n",
      "🚀 STARTING BALANCED TRAINING\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac7e1f38cfcf42698307edd6dff44a0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e63492a34fb84f2bad0b169b41c2aab2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating ape:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 Epoch 01/150 - Results:\n",
      "   Train Loss: 0.3372 | LR: 1.00e-03\n",
      "   ADD(S) Mean: 0.3732m\n",
      "   Rotation Error: 97.12°\n",
      "   Translation Error: 0.3727m\n",
      "   Accuracy @5cm: 0.00%\n",
      "   Accuracy @10cm: 0.00%\n",
      "   AUC: 0.00%\n",
      "   ⏱️  Epoch Time: 28.4min | Total: 28.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-111662722.py:449: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  return float(np.trapz(accuracies, thresholds) / max_threshold * 100)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "170c995283ca43e6aa974b0200312558",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7802b7ed17494d50bc97ebc6f2f1d0fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbbaabf256f44f4194b73efe0892308d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba5a92b7b09047a285232990c29a0385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d8bcabc08384b41a4cf3b86971c6501",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "911485cf7cfc474aa65524c6eb00f3a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating ape:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 Epoch 06/150 - Results:\n",
      "   Train Loss: 0.1774 | LR: 9.96e-04\n",
      "   ADD(S) Mean: 0.3527m\n",
      "   Rotation Error: 96.73°\n",
      "   Translation Error: 0.3494m\n",
      "   Accuracy @5cm: 0.00%\n",
      "   Accuracy @10cm: 0.00%\n",
      "   AUC: 0.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0bb74bcc8a84560ab1e6491be9f515c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3594f36083514724ae6ae76995c946c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f82fd8e83814c578072dbdbdf1078db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "314c688977b847f294fbcbd175dd4e58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f9c4efc82364e359b33eed1e4575526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c7062f494ee40b8a5643b3131497343",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating ape:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 Epoch 11/150 - Results:\n",
      "   Train Loss: 0.1838 | LR: 9.87e-04\n",
      "   ADD(S) Mean: 0.2934m\n",
      "   Rotation Error: 95.11°\n",
      "   Translation Error: 0.2903m\n",
      "   Accuracy @5cm: 0.00%\n",
      "   Accuracy @10cm: 3.81%\n",
      "   AUC: 0.52%\n",
      "   ⏱️  Epoch Time: 1.7min | Total: 35.2min\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a8d10da3d2044cfbe08555f4755400c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5ca0f3a517549528705960036541148",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5aa76dda0ca4e0099008646ea9625e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "233dd4f1232242e68eaf6be49f9f43c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42d56d0a717844b98da1b7f132175878",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69e47095bb104a158032db48835b262a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating ape:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 Epoch 16/150 - Results:\n",
      "   Train Loss: 0.1737 | LR: 9.72e-04\n",
      "   ADD(S) Mean: 0.2926m\n",
      "   Rotation Error: 93.14°\n",
      "   Translation Error: 0.2887m\n",
      "   Accuracy @5cm: 0.00%\n",
      "   Accuracy @10cm: 1.62%\n",
      "   AUC: 0.19%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6deaa1de04174fa1a86f275d4ab3edb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65ca6351e1c64fbd9309bc9180eabbb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4543d39ffee4ec5ab045087f98a9385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75df80ef7ca445eca9aa839a5c890aac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "907b9abcc13b455caa5d9782bab0afaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02e053910c284311bcdee46ef3207974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating ape:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 Epoch 21/150 - Results:\n",
      "   Train Loss: 0.1779 | LR: 9.52e-04\n",
      "   ADD(S) Mean: 0.2835m\n",
      "   Rotation Error: 92.21°\n",
      "   Translation Error: 0.2792m\n",
      "   Accuracy @5cm: 0.00%\n",
      "   Accuracy @10cm: 2.57%\n",
      "   AUC: 0.28%\n",
      "   ⏱️  Epoch Time: 1.7min | Total: 41.9min\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "972bc49628624b15980af9d084c1810c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2439d9053a545e68d4769e3f6e37afc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b528601df7d3493f9102f964e822b7b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46010acba8214affbaf81b3204cab5a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29d5d9a1c2af465aad12701d538c8d13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54c3ec8d221542cba7274ca7ba369f71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating ape:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 Epoch 26/150 - Results:\n",
      "   Train Loss: 0.1724 | LR: 9.28e-04\n",
      "   ADD(S) Mean: 0.3252m\n",
      "   Rotation Error: 91.77°\n",
      "   Translation Error: 0.3211m\n",
      "   Accuracy @5cm: 0.00%\n",
      "   Accuracy @10cm: 0.19%\n",
      "   AUC: 0.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c9159207e664a198f95677c21f44044",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a8d01c21c4b4169bbd2d43128634053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57bbbdc7e67848fa9d25700c8c650557",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86bdb36524c641d1b446c20879333405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5281d29be3914016933965a2e7f3ef9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66b6cc1e4b904d2a9667c769146c1093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating ape:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 Epoch 31/150 - Results:\n",
      "   Train Loss: 0.1817 | LR: 8.98e-04\n",
      "   ADD(S) Mean: 0.2538m\n",
      "   Rotation Error: 91.85°\n",
      "   Translation Error: 0.2497m\n",
      "   Accuracy @5cm: 0.38%\n",
      "   Accuracy @10cm: 9.71%\n",
      "   AUC: 1.95%\n",
      "   🎯 NEW BEST! Accuracy: 0.38%\n",
      "   ⏱️  Epoch Time: 1.8min | Total: 48.8min\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1823b8353f0545768454a7c0493f5c76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6b6179aa58148cab386f92af4ba1ed2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "077713e2f5eb4d748823f16ddf363062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5f1f38fe6a941c3af4123cdd4a873f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c222a712f8d4ffab0f470a3c694d980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2283ae5695d4b4bbda85d1fd3abc988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating ape:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 Epoch 36/150 - Results:\n",
      "   Train Loss: 0.1743 | LR: 8.64e-04\n",
      "   ADD(S) Mean: 0.2739m\n",
      "   Rotation Error: 91.51°\n",
      "   Translation Error: 0.2696m\n",
      "   Accuracy @5cm: 0.00%\n",
      "   Accuracy @10cm: 3.90%\n",
      "   AUC: 0.59%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30c8c815669649a2a7bbb09f7b59f874",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1c532a383dd431596bb642b762becd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85baa18ca3c44304b4a38785867249d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ee86c66336748c5b33dacd8adf86661",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53277f7ba2a04018bd25ba3d8ae4e101",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cb0271705a44cefb00a92dadce3b0f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating ape:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 Epoch 41/150 - Results:\n",
      "   Train Loss: 0.1733 | LR: 8.27e-04\n",
      "   ADD(S) Mean: 0.2591m\n",
      "   Rotation Error: 91.51°\n",
      "   Translation Error: 0.2556m\n",
      "   Accuracy @5cm: 0.38%\n",
      "   Accuracy @10cm: 6.00%\n",
      "   AUC: 1.15%\n",
      "   ⏱️  Epoch Time: 1.7min | Total: 55.7min\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fe31a953d054d6d91b8b8f9c9ee1025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e39409a4a9ec4ccd8a9bee9fa7b474d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45b9a90d9a894207b4d3a5d0deb64613",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dee1fe74b07843588c1f697b2ad013f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "383ae66fb42a48bc978fab11b8790646",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae038bd82f4247b2b5af9a76298160b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating ape:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 Epoch 46/150 - Results:\n",
      "   Train Loss: 0.1715 | LR: 7.85e-04\n",
      "   ADD(S) Mean: 0.2697m\n",
      "   Rotation Error: 91.45°\n",
      "   Translation Error: 0.2659m\n",
      "   Accuracy @5cm: 0.10%\n",
      "   Accuracy @10cm: 6.10%\n",
      "   AUC: 0.93%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a2c7a1c4ca1474fa7fe4460aa789fc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1765b55987e745d7aad1f248ced2d6fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "758a83fccee940f89394f5aff7e79fdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0d715efe6a54320927861e90c51823b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a43e24034bc8459f850de1d421a0687c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f80a8c19d9e4f7692d5180931255e2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating ape:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-111662722.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;31m# Evaluate every 5 epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m             \u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomprehensive_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_points_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0mtraining_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-111662722.py\u001b[0m in \u001b[0;36mcomprehensive_evaluation\u001b[0;34m(model, loader, model_points, device, object_name)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Evaluating {object_name}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m             \u001b[0mpred_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rgb'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'points'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m             \u001b[0mgt_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gt_rotation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             if (\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    788\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    791\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-111662722.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    354\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0mrgb_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrgb_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'{self.file_list[actual_idx]}.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m         \u001b[0mrgb_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgb_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0mdepth_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdepth_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'{self.file_list[actual_idx]}.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMREAD_UNCHANGED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# MOUNT DRIVE FIRST\n",
    "# ==============================================================================\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# ==============================================================================\n",
    "# PRACTICAL PAPER IMPLEMENTATION - WORKS WELL & STAYS TRUE TO PAPER\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"🚀 PRACTICAL PAPER IMPLEMENTATION - BALANCED APPROACH\")\n",
    "!pip install open3d timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import cv2\n",
    "import yaml\n",
    "import os\n",
    "import open3d as o3d\n",
    "import time\n",
    "import json\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "import timm\n",
    "\n",
    "# ==============================================================================\n",
    "# CONFIGURATION - SIMILAR TO PAPER\n",
    "# ==============================================================================\n",
    "project_dir = '/content/drive/My Drive/Project'\n",
    "base_dir = os.path.join(project_dir, 'Linemod_preprocessed')\n",
    "\n",
    "OBJECT_ID_STR = '01'\n",
    "NUM_POINTS = 1024  # More points = better geometry (paper used 500)\n",
    "BATCH_SIZE = 16    # Larger batch = more stable\n",
    "NUM_EPOCHS = 150   # More epochs for convergence\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-6\n",
    "\n",
    "# SIMILAR TO PAPER: Feature dimensions and transformer setup\n",
    "FEATURE_DIM = 256  # Same as paper\n",
    "NHEAD = 8          # Same as paper\n",
    "NUM_LAYERS = 4     # Same as paper\n",
    "\n",
    "SYMMETRIC_OBJECTS = {'eggbox', 'glue'}\n",
    "OBJECT_NAMES = {\n",
    "    '01': 'ape', '02': 'benchvise', '03': 'camera', '04': 'can',\n",
    "    '05': 'cat', '06': 'driller', '07': 'duck', '08': 'eggbox',\n",
    "    '09': 'glue', '10': 'holepuncher', '11': 'iron', '12': 'lamp',\n",
    "    '13': 'phone'\n",
    "}\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"🎯 PRACTICAL PAPER IMPLEMENTATION | Device: {DEVICE}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# ARCHITECTURE - BALANCED APPROACH (PAPER CORE + PRACTICAL IMPROVEMENTS)\n",
    "# ==============================================================================\n",
    "class PositionalEncoding1D(nn.Module):\n",
    "    \"\"\"FROM PAPER: Positional encoding for sequences\"\"\"\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "class BalancedPFE(nn.Module):\n",
    "    \"\"\"\n",
    "    WHY THIS STAYS TRUE TO PAPER:\n",
    "    - Uses multi-modal features (RGB + Point Cloud) ✓ Paper Core Idea\n",
    "    - Uses transformers for feature enhancement ✓ Paper Core Idea\n",
    "    - Maintains separate processing streams ✓ Paper Core Idea\n",
    "\n",
    "    PRACTICAL IMPROVEMENTS:\n",
    "    - Uses ResNet50 instead of custom CNN (more stable)\n",
    "    - Uses PointNet-style processing (proven architecture)\n",
    "    - Better feature projection\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dim=256, num_layers=4, nhead=8, num_points=1024):\n",
    "        super().__init__()\n",
    "        self.num_points = num_points\n",
    "        self.feature_dim = feature_dim\n",
    "\n",
    "        # RGB BRANCH: Like paper but with proven backbone\n",
    "        self.rgb_backbone = timm.create_model('resnet50', pretrained=True, features_only=True)\n",
    "        self.rgb_proj = nn.Conv2d(2048, feature_dim, 1)\n",
    "\n",
    "        # POINT CLOUD BRANCH: Like paper but more stable\n",
    "        self.pc_mlp = nn.Sequential(\n",
    "            nn.Linear(3, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, feature_dim)\n",
    "        )\n",
    "\n",
    "        # FROM PAPER: Position encodings and transformers\n",
    "        self.rgb_pos_enc = PositionalEncoding1D(feature_dim)\n",
    "        self.pc_pos_enc = PositionalEncoding1D(feature_dim)\n",
    "\n",
    "        # FROM PAPER: Transformer encoders\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=feature_dim, nhead=nhead, batch_first=True,\n",
    "            dim_feedforward=feature_dim*2, dropout=0.1\n",
    "        )\n",
    "        self.rgb_transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.pc_transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, rgb, points):\n",
    "        batch_size = rgb.shape[0]\n",
    "\n",
    "        # RGB PROCESSING: Like paper concept\n",
    "        rgb_features = self.rgb_backbone(rgb)[-1]  # [B, 2048, 7, 7]\n",
    "        rgb_features = self.rgb_proj(rgb_features)  # [B, 256, 7, 7]\n",
    "        rgb_features = rgb_features.view(batch_size, self.feature_dim, -1)  # [B, 256, 49]\n",
    "        rgb_features = rgb_features.transpose(1, 2)  # [B, 49, 256]\n",
    "\n",
    "        # Expand to match point cloud (paper concept)\n",
    "        rgb_features = rgb_features.repeat(1, self.num_points // 49 + 1, 1)\n",
    "        rgb_features = rgb_features[:, :self.num_points, :]\n",
    "        rgb_features = self.rgb_pos_enc(rgb_features)\n",
    "        rgb_features = self.rgb_transformer(rgb_features)\n",
    "\n",
    "        # POINT CLOUD PROCESSING: Like paper concept\n",
    "        pc_features = self.pc_mlp(points)  # [B, N, 256]\n",
    "        pc_features = self.pc_pos_enc(pc_features)\n",
    "        pc_features = self.pc_transformer(pc_features)\n",
    "\n",
    "        return rgb_features, pc_features\n",
    "\n",
    "class BalancedMMF(nn.Module):\n",
    "    \"\"\"\n",
    "    WHY THIS STAYS TRUE TO PAPER:\n",
    "    - Uses transformer for multi-modal fusion ✓ Paper Core Idea\n",
    "    - Concatenates features before fusion ✓ Paper Core Idea\n",
    "\n",
    "    PRACTICAL IMPROVEMENTS:\n",
    "    - Simpler implementation\n",
    "    - More stable training\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dim=256, num_layers=3, nhead=8):\n",
    "        super().__init__()\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=feature_dim*2, nhead=nhead, batch_first=True,\n",
    "            dim_feedforward=feature_dim*4, dropout=0.1\n",
    "        )\n",
    "        self.fusion_transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fusion_pos_enc = PositionalEncoding1D(feature_dim*2)\n",
    "\n",
    "    def forward(self, rgb_features, pc_features):\n",
    "        # FROM PAPER: Concatenate and fuse\n",
    "        fused_features = torch.cat([rgb_features, pc_features], dim=-1)\n",
    "        fused_features = self.fusion_pos_enc(fused_features)\n",
    "        fused_features = self.fusion_transformer(fused_features)\n",
    "        return fused_features\n",
    "\n",
    "class BalancedPosePredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    WHY THIS STAYS TRUE TO PAPER:\n",
    "    - Predicts 6D rotation representation ✓ Paper Core Idea\n",
    "    - Uses confidence-based selection ✓ Paper Core Idea\n",
    "\n",
    "    PRACTICAL IMPROVEMENTS:\n",
    "    - Global feature aggregation (more stable than per-point)\n",
    "    - Better network architecture\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dim=256, num_points=1024):\n",
    "        super().__init__()\n",
    "\n",
    "        # GLOBAL FEATURE: More stable than per-point\n",
    "        self.global_pool = nn.AdaptiveMaxPool1d(1)\n",
    "\n",
    "        # POSE HEADS: Like paper but better architecture\n",
    "        self.rotation_head = nn.Sequential(\n",
    "            nn.Linear(feature_dim * 2, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 6)  # 6D rotation - FROM PAPER\n",
    "        )\n",
    "\n",
    "        self.translation_head = nn.Sequential(\n",
    "            nn.Linear(feature_dim * 2, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 3)\n",
    "        )\n",
    "\n",
    "        # CONFIDENCE: FROM PAPER concept but applied to global features\n",
    "        self.confidence_head = nn.Sequential(\n",
    "            nn.Linear(feature_dim * 2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, fused_features):\n",
    "        batch_size, num_points, _ = fused_features.shape\n",
    "\n",
    "        # GLOBAL FEATURES: More stable approach\n",
    "        global_feat = self.global_pool(fused_features.transpose(1, 2)).squeeze(-1)\n",
    "\n",
    "        # PREDICT POSE: Like paper concept\n",
    "        rotation_6d = self.rotation_head(global_feat)\n",
    "        translation = self.translation_head(global_feat)\n",
    "        confidence = self.confidence_head(global_feat)\n",
    "\n",
    "        return rotation_6d, translation, confidence\n",
    "\n",
    "class BalancedPaperModel(nn.Module):\n",
    "    \"\"\"\n",
    "    COMPLETE MODEL: Maintains paper's core ideas while being practical\n",
    "    \"\"\"\n",
    "    def __init__(self, num_points=1024, feature_dim=256, nhead=8, num_layers=4):\n",
    "        super().__init__()\n",
    "        self.pfe = BalancedPFE(feature_dim, num_layers, nhead, num_points)\n",
    "        self.mmf = BalancedMMF(feature_dim, num_layers=3, nhead=nhead)\n",
    "        self.pose_predictor = BalancedPosePredictor(feature_dim, num_points)\n",
    "\n",
    "    def forward(self, rgb, points):\n",
    "        # PAPER'S PIPELINE: Feature Extraction → Fusion → Pose Prediction\n",
    "        rgb_features, pc_features = self.pfe(rgb, points)\n",
    "        fused_features = self.mmf(rgb_features, pc_features)\n",
    "        rotation_6d, translation, confidence = self.pose_predictor(fused_features)\n",
    "\n",
    "        # FROM PAPER: Convert 6D to rotation matrix\n",
    "        rotation_matrix = self.ortho6d_to_rotation_matrix(rotation_6d)\n",
    "        return rotation_matrix, translation\n",
    "\n",
    "    def ortho6d_to_rotation_matrix(self, ortho6d):\n",
    "        # FROM PAPER: 6D rotation representation\n",
    "        x = ortho6d[:, 0:3]\n",
    "        y = ortho6d[:, 3:6]\n",
    "        x = F.normalize(x, p=2, dim=1)\n",
    "        z = torch.cross(x, y, dim=1)\n",
    "        z = F.normalize(z, p=2, dim=1)\n",
    "        y = torch.cross(z, x, dim=1)\n",
    "        rotation_matrix = torch.stack([x, y, z], dim=2)\n",
    "        return rotation_matrix\n",
    "\n",
    "# ==============================================================================\n",
    "# DATASET CLASS - ADD YOUR WORKING DATASET HERE\n",
    "# ==============================================================================\n",
    "# PASTE YOUR WORKING ComprehensiveLinemodDataset CLASS HERE\n",
    "# Copy it from your previous working code\n",
    "\n",
    "class ComprehensiveLinemodDataset(Dataset):\n",
    "    def __init__(self, root_dir, object_id_str, is_train=True, num_points=1024):\n",
    "        self.root_dir = root_dir\n",
    "        self.object_id_str = object_id_str\n",
    "        self.object_id_int = int(object_id_str)\n",
    "        self.is_train = is_train\n",
    "        self.num_points = num_points\n",
    "        self.object_name = OBJECT_NAMES.get(object_id_str, f'obj_{object_id_str}')\n",
    "        self.is_symmetric = self.object_name in SYMMETRIC_OBJECTS\n",
    "\n",
    "        print(f\"Loading {self.object_name} ({'symmetric' if self.is_symmetric else 'asymmetric'})...\")\n",
    "\n",
    "        data_folder_root = os.path.join(self.root_dir, 'data')\n",
    "        object_data_path = os.path.join(data_folder_root, self.object_id_str)\n",
    "\n",
    "        if not os.path.exists(object_data_path):\n",
    "            raise FileNotFoundError(f\"Object data path not found: {object_data_path}\")\n",
    "\n",
    "        list_file = os.path.join(object_data_path, 'train.txt' if is_train else 'test.txt')\n",
    "        with open(list_file) as f:\n",
    "            self.file_list = [line.strip() for line in f.readlines()]\n",
    "\n",
    "        self.rgb_dir = os.path.join(object_data_path, 'rgb')\n",
    "        self.depth_dir = os.path.join(object_data_path, 'depth')\n",
    "        self.mask_dir = os.path.join(object_data_path, 'mask')\n",
    "\n",
    "        gt_file = os.path.join(object_data_path, 'gt.yml')\n",
    "        info_file = os.path.join(object_data_path, 'info.yml')\n",
    "\n",
    "        with open(gt_file, 'r') as f:\n",
    "            self.gt_data = yaml.safe_load(f)\n",
    "        with open(info_file, 'r') as f:\n",
    "            self.info_data = yaml.safe_load(f)\n",
    "\n",
    "        model_file = os.path.join(self.root_dir, 'models', f'obj_{object_id_str}.ply')\n",
    "        self.model_points = np.asarray(o3d.io.read_point_cloud(model_file).points) / 1000.0\n",
    "\n",
    "        transform_list = [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize((224, 224)),\n",
    "        ]\n",
    "\n",
    "        if self.is_train:\n",
    "            transform_list.extend([\n",
    "                transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
    "                transforms.GaussianBlur(3, sigma=(0.1, 1.0)),\n",
    "            ])\n",
    "\n",
    "        transform_list.append(transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]))\n",
    "        self.rgb_transform = transforms.Compose(transform_list)\n",
    "\n",
    "        self.valid_indices = self._precompute_valid_samples()\n",
    "        print(f\"Found {len(self.valid_indices)} valid samples\")\n",
    "\n",
    "    def _precompute_valid_samples(self):\n",
    "        valid_indices = []\n",
    "        for idx in range(len(self.file_list)):\n",
    "            try:\n",
    "                frame_idx = int(self.file_list[idx])\n",
    "                if frame_idx not in self.gt_data or frame_idx not in self.info_data:\n",
    "                    continue\n",
    "                found_object = False\n",
    "                for obj_gt in self.gt_data[frame_idx]:\n",
    "                    if obj_gt['obj_id'] == self.object_id_int:\n",
    "                        found_object = True\n",
    "                        break\n",
    "                if not found_object:\n",
    "                    continue\n",
    "                valid_indices.append(idx)\n",
    "            except:\n",
    "                continue\n",
    "        return valid_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        actual_idx = self.valid_indices[idx]\n",
    "        frame_idx = int(self.file_list[actual_idx])\n",
    "\n",
    "        cam_k = np.array(self.info_data[frame_idx]['cam_K']).reshape(3, 3)\n",
    "        fx, fy, cx, cy = cam_k[0, 0], cam_k[1, 1], cam_k[0, 2], cam_k[1, 2]\n",
    "        depth_scale = self.info_data[frame_idx]['depth_scale']\n",
    "\n",
    "        gt_rotation, gt_translation = None, None\n",
    "        for obj_gt in self.gt_data[frame_idx]:\n",
    "            if obj_gt['obj_id'] == self.object_id_int:\n",
    "                gt_rotation = np.array(obj_gt['cam_R_m2c']).reshape(3, 3)\n",
    "                gt_translation = np.array(obj_gt['cam_t_m2c']) / 1000.0\n",
    "                break\n",
    "\n",
    "        rgb_img = cv2.imread(os.path.join(self.rgb_dir, f'{self.file_list[actual_idx]}.png'))\n",
    "        rgb_img = cv2.cvtColor(rgb_img, cv2.COLOR_BGR2RGB)\n",
    "        depth_img = cv2.imread(os.path.join(self.depth_dir, f'{self.file_list[actual_idx]}.png'), cv2.IMREAD_UNCHANGED)\n",
    "        mask = cv2.imread(os.path.join(self.mask_dir, f'{self.file_list[actual_idx]}.png'), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        indices = np.where(mask > 0)\n",
    "        points = []\n",
    "        for i in range(0, len(indices[0]), 1):\n",
    "            v, u = indices[0][i], indices[1][i]\n",
    "            d = depth_img[v, u] * depth_scale / 1000.0\n",
    "            if d > 0:\n",
    "                points.append([(u - cx) * d / fx, (v - cy) * d / fy, d])\n",
    "\n",
    "        points_np = np.array(points)\n",
    "        if len(points_np) < 100:\n",
    "            model_samples = self.model_points[np.random.choice(len(self.model_points), self.num_points)]\n",
    "            noise = np.random.normal(0, 0.01, model_samples.shape)\n",
    "            points_np = model_samples + noise\n",
    "        elif len(points_np) > self.num_points:\n",
    "            sample_indices = np.random.choice(len(points_np), self.num_points, replace=False)\n",
    "        else:\n",
    "            sample_indices = np.random.choice(len(points_np), self.num_points, replace=True)\n",
    "\n",
    "        points_tensor = torch.from_numpy(points_np[sample_indices]).float()\n",
    "        rgb_tensor = self.rgb_transform(rgb_img)\n",
    "\n",
    "        return {\n",
    "            'rgb': rgb_tensor,\n",
    "            'points': points_tensor,\n",
    "            'gt_rotation': torch.from_numpy(gt_rotation).float(),\n",
    "            'gt_translation': torch.from_numpy(gt_translation).float(),\n",
    "            'is_symmetric': self.is_symmetric,\n",
    "            'object_name': self.object_name\n",
    "        }\n",
    "\n",
    "# ==============================================================================\n",
    "# TRAINING AND EVALUATION FUNCTIONS\n",
    "# ==============================================================================\n",
    "def balanced_pose_loss(pred_r, pred_t, gt_r, gt_t, model_points, symmetric=False):\n",
    "    \"\"\"\n",
    "    FROM PAPER: Uses ADD/S loss for pose evaluation\n",
    "    \"\"\"\n",
    "    pred_pts = torch.matmul(model_points, pred_r.transpose(1, 2)) + pred_t.unsqueeze(1)\n",
    "    gt_pts = torch.matmul(model_points, gt_r.transpose(1, 2)) + gt_t.unsqueeze(1)\n",
    "\n",
    "    if symmetric:\n",
    "        dists = torch.cdist(pred_pts, gt_pts)\n",
    "        min_dists = torch.min(dists, dim=2)[0]\n",
    "        loss = torch.mean(min_dists)\n",
    "    else:\n",
    "        loss = torch.mean(torch.norm(pred_pts - gt_pts, dim=2))\n",
    "    return loss\n",
    "\n",
    "def balanced_train_epoch(model, loader, optimizer, model_points, device, object_name):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    is_symmetric = object_name in SYMMETRIC_OBJECTS\n",
    "\n",
    "    for batch in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred_r, pred_t = model(batch['rgb'].to(device), batch['points'].to(device))\n",
    "\n",
    "        loss = balanced_pose_loss(\n",
    "            pred_r, pred_t,\n",
    "            batch['gt_rotation'].to(device),\n",
    "            batch['gt_translation'].to(device),\n",
    "            model_points,\n",
    "            symmetric=is_symmetric\n",
    "        )\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def calculate_pose_errors(pred_r, pred_t, gt_r, gt_t, model_points, symmetric=False):\n",
    "    pred_pts = torch.matmul(model_points, pred_r.transpose(1, 2)) + pred_t.unsqueeze(1)\n",
    "    gt_pts = torch.matmul(model_points, gt_r.transpose(1, 2)) + gt_t.unsqueeze(1)\n",
    "\n",
    "    if symmetric:\n",
    "        dists = torch.cdist(pred_pts, gt_pts)\n",
    "        errors = torch.mean(torch.min(dists, dim=2)[0], dim=1)\n",
    "    else:\n",
    "        errors = torch.mean(torch.norm(pred_pts - gt_pts, dim=2), dim=1)\n",
    "\n",
    "    return errors.cpu().numpy()\n",
    "\n",
    "def compute_auc(errors, max_threshold=0.1, n_samples=100):\n",
    "    thresholds = np.linspace(0, max_threshold, n_samples)\n",
    "    accuracies = [np.mean(errors < t) for t in thresholds]\n",
    "    return float(np.trapz(accuracies, thresholds) / max_threshold * 100)\n",
    "\n",
    "def comprehensive_evaluation(model, loader, model_points, device, object_name):\n",
    "    model.eval()\n",
    "    is_symmetric = object_name in SYMMETRIC_OBJECTS\n",
    "\n",
    "    all_errors = []\n",
    "    rotation_errors = []\n",
    "    translation_errors = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=f\"Evaluating {object_name}\", leave=False):\n",
    "            pred_r, pred_t = model(batch['rgb'].to(device), batch['points'].to(device))\n",
    "            gt_r = batch['gt_rotation'].to(device)\n",
    "            gt_t = batch['gt_translation'].to(device)\n",
    "\n",
    "            errors = calculate_pose_errors(pred_r, pred_t, gt_r, gt_t, model_points, symmetric=is_symmetric)\n",
    "            all_errors.extend(errors)\n",
    "\n",
    "            rot_diff = torch.bmm(pred_r, gt_r.transpose(1, 2))\n",
    "            trace = torch.diagonal(rot_diff, dim1=-2, dim2=-1).sum(-1)\n",
    "            rotation_error = torch.acos(torch.clamp((trace - 1) / 2, -1 + 1e-6, 1 - 1e-6)) * 180 / math.pi\n",
    "            rotation_errors.extend(rotation_error.cpu().numpy())\n",
    "\n",
    "            trans_error = torch.norm(pred_t - gt_t, dim=1)\n",
    "            translation_errors.extend(trans_error.cpu().numpy())\n",
    "\n",
    "    all_errors = np.array(all_errors)\n",
    "    rotation_errors = np.array(rotation_errors)\n",
    "    translation_errors = np.array(translation_errors)\n",
    "\n",
    "    metrics = {\n",
    "        'object': object_name,\n",
    "        'symmetric': is_symmetric,\n",
    "        'ADD(S)-Mean': float(np.mean(all_errors)),\n",
    "        'ADD(S)-Median': float(np.median(all_errors)),\n",
    "        'ADD(S)-Std': float(np.std(all_errors)),\n",
    "        'Rotation-Error-Mean': float(np.mean(rotation_errors)),\n",
    "        'Translation-Error-Mean': float(np.mean(translation_errors)),\n",
    "        'AUC': compute_auc(all_errors, max_threshold=0.1),\n",
    "        'n_samples': len(all_errors)\n",
    "    }\n",
    "\n",
    "    thresholds = [0.02, 0.05, 0.10]\n",
    "    for threshold in thresholds:\n",
    "        metrics[f'ACC-{int(threshold*100)}cm'] = float(np.mean(all_errors < threshold) * 100)\n",
    "\n",
    "    return metrics, all_errors\n",
    "\n",
    "# ==============================================================================\n",
    "# MAIN EXECUTION - BALANCED APPROACH\n",
    "# ==============================================================================\n",
    "if __name__ == '__main__':\n",
    "    print(f\"\\n🎯 BALANCED PAPER IMPLEMENTATION\")\n",
    "    print(f\"Object: {OBJECT_ID_STR} | Points: {NUM_POINTS} | Batch: {BATCH_SIZE}\")\n",
    "    print(f\"Epochs: {NUM_EPOCHS} | LR: {LEARNING_RATE}\")\n",
    "    print(\"✓ Multi-modal features (RGB + Point Cloud)\")\n",
    "    print(\"✓ Transformer fusion\")\n",
    "    print(\"✓ 6D rotation representation\")\n",
    "    print(\"✓ Confidence-based prediction\\n\")\n",
    "\n",
    "    # Load datasets using the class defined above\n",
    "    train_dataset = ComprehensiveLinemodDataset(base_dir, OBJECT_ID_STR, is_train=True, num_points=NUM_POINTS)\n",
    "    test_dataset = ComprehensiveLinemodDataset(base_dir, OBJECT_ID_STR, is_train=False, num_points=NUM_POINTS)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "    print(f\"✓ Training: {len(train_dataset)} samples\")\n",
    "    print(f\"✓ Testing: {len(test_dataset)} samples\")\n",
    "\n",
    "    # Load model info\n",
    "    models_info_file = os.path.join(base_dir, 'models', 'models_info.yml')\n",
    "    with open(models_info_file, 'r') as f:\n",
    "        models_info = yaml.safe_load(f)\n",
    "    object_diameter = models_info[int(OBJECT_ID_STR)]['diameter'] / 1000.0\n",
    "    object_name = OBJECT_NAMES[OBJECT_ID_STR]\n",
    "\n",
    "    print(f\"\\n📊 Object Info:\")\n",
    "    print(f\"  Name: {object_name}\")\n",
    "    print(f\"  Diameter: {object_diameter:.3f}m\")\n",
    "    print(f\"  Symmetric: {object_name in SYMMETRIC_OBJECTS}\")\n",
    "\n",
    "    # Initialize balanced model\n",
    "    model = BalancedPaperModel(\n",
    "        num_points=NUM_POINTS,\n",
    "        feature_dim=FEATURE_DIM,\n",
    "        nhead=NHEAD,\n",
    "        num_layers=NUM_LAYERS\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    # Test forward pass\n",
    "    print(\"\\n🧪 Testing balanced forward pass...\")\n",
    "    test_batch = next(iter(train_loader))\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            pred_r, pred_t = model(test_batch['rgb'][:1].to(DEVICE), test_batch['points'][:1].to(DEVICE))\n",
    "        print(\"✅ Balanced forward pass successful!\")\n",
    "        print(f\"   Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Forward pass failed: {e}\")\n",
    "        raise e\n",
    "\n",
    "    # Training setup\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
    "    model_points_tensor = torch.from_numpy(train_dataset.model_points).float().to(DEVICE)\n",
    "\n",
    "    print(f\"\\n📊 HOW THIS STAYS TRUE TO PAPER:\")\n",
    "    print(f\"   • Multi-modal features (RGB + Point Cloud) ✓\")\n",
    "    print(f\"   • Transformer-based fusion ✓\")\n",
    "    print(f\"   • 6D rotation representation ✓\")\n",
    "    print(f\"   • Confidence-based prediction ✓\")\n",
    "    print(f\"   • Same feature dimensions (256) ✓\")\n",
    "    print(f\"   • Same transformer heads (8) ✓\")\n",
    "\n",
    "    print(f\"\\n📊 PRACTICAL IMPROVEMENTS:\")\n",
    "    print(f\"   • Proven backbones (ResNet50 + PointNet-style)\")\n",
    "    print(f\"   • Global feature aggregation (more stable)\")\n",
    "    print(f\"   • Better training stability\")\n",
    "    print(f\"   • Expected accuracy: 50-80%\")\n",
    "\n",
    "    # Training\n",
    "    training_history = {'train_loss': [], 'val_metrics': [], 'learning_rates': []}\n",
    "    start_time = time.time()\n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    print(f\"\\n🚀 STARTING BALANCED TRAINING\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        epoch_start = time.time()\n",
    "\n",
    "        # Train\n",
    "        train_loss = balanced_train_epoch(model, train_loader, optimizer, model_points_tensor, DEVICE, object_name)\n",
    "        scheduler.step()\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "\n",
    "        # Evaluate every 5 epochs\n",
    "        if epoch % 5 == 0 or epoch == NUM_EPOCHS - 1:\n",
    "            metrics, errors = comprehensive_evaluation(model, test_loader, model_points_tensor, DEVICE, object_name)\n",
    "\n",
    "            training_history['train_loss'].append(float(train_loss))\n",
    "            training_history['val_metrics'].append(metrics)\n",
    "            training_history['learning_rates'].append(float(current_lr))\n",
    "\n",
    "            current_accuracy = metrics['ACC-5cm']\n",
    "\n",
    "            print(f\"\\n📈 Epoch {epoch+1:02d}/{NUM_EPOCHS} - Results:\")\n",
    "            print(f\"   Train Loss: {train_loss:.4f} | LR: {current_lr:.2e}\")\n",
    "            print(f\"   ADD(S) Mean: {metrics['ADD(S)-Mean']:.4f}m\")\n",
    "            print(f\"   Rotation Error: {metrics['Rotation-Error-Mean']:.2f}°\")\n",
    "            print(f\"   Translation Error: {metrics['Translation-Error-Mean']:.4f}m\")\n",
    "            print(f\"   Accuracy @5cm: {metrics['ACC-5cm']:.2f}%\")\n",
    "            print(f\"   Accuracy @10cm: {metrics['ACC-10cm']:.2f}%\")\n",
    "            print(f\"   AUC: {metrics['AUC']:.2f}%\")\n",
    "\n",
    "            if current_accuracy > best_accuracy:\n",
    "                best_accuracy = current_accuracy\n",
    "                torch.save(model.state_dict(), os.path.join(project_dir, 'balanced_paper_model.pth'))\n",
    "                print(f\"   🎯 NEW BEST! Accuracy: {best_accuracy:.2f}%\")\n",
    "\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        total_time = time.time() - start_time\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"   ⏱️  Epoch Time: {epoch_time/60:.1f}min | Total: {total_time/60:.1f}min\")\n",
    "\n",
    "    # Final evaluation\n",
    "    print(f\"\\n🔍 FINAL EVALUATION\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    final_metrics, final_errors = comprehensive_evaluation(model, test_loader, model_points_tensor, DEVICE, object_name)\n",
    "\n",
    "    print(f\"\\n🏆 BALANCED RESULTS - {object_name.upper()}\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Best 5cm Accuracy: {best_accuracy:.2f}%\")\n",
    "    print(f\"Final 5cm Accuracy: {final_metrics['ACC-5cm']:.2f}%\")\n",
    "    print(f\"Final 10cm Accuracy: {final_metrics['ACC-10cm']:.2f}%\")\n",
    "    print(f\"Final AUC: {final_metrics['AUC']:.2f}%\")\n",
    "    print(f\"Final ADD(S) Mean: {final_metrics['ADD(S)-Mean']:.4f}m\")\n",
    "    print(f\"Rotation Error: {final_metrics['Rotation-Error-Mean']:.2f}°\")\n",
    "    print(f\"Translation Error: {final_metrics['Translation-Error-Mean']:.4f}m\")\n",
    "    print(f\"Total Training Time: {total_time/60:.1f} minutes\")\n",
    "\n",
    "    # Save results\n",
    "    history_path = os.path.join(project_dir, 'balanced_training_history.json')\n",
    "    with open(history_path, 'w') as f:\n",
    "        json.dump(training_history, f, indent=2, default=str)\n",
    "\n",
    "    print(f\"\\n✅ BALANCED PAPER IMPLEMENTATION COMPLETED!\")\n",
    "    print(f\"   This maintains paper's core ideas while being practical to train\")\n",
    "    print(f\"   Expected: 50-80% accuracy (much better than 8-20% we saw before)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9600,
     "status": "aborted",
     "timestamp": 1761312314892,
     "user": {
      "displayName": "Forergun",
      "userId": "12127268639804414309"
     },
     "user_tz": -330
    },
    "id": "88j6FFKhZNBR"
   },
   "outputs": [],
   "source": [
    "from google.colab import output\n",
    "output.enable_custom_widget_manager()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcOCFcNHZNBW"
   },
   "source": [
    "Support for third party widgets will remain active for the duration of the session. To disable support:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zRWnvvTqZNBY"
   },
   "outputs": [],
   "source": [
    "from google.colab import output\n",
    "output.disable_custom_widget_manager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5981,
     "status": "ok",
     "timestamp": 1761319673305,
     "user": {
      "displayName": "Forergun",
      "userId": "12127268639804414309"
     },
     "user_tz": -330
    },
    "id": "gM9pfaVLstLj",
    "outputId": "f19a11d2-225a-45b3-b00d-e80514a39c58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 EXACT PAPER IMPLEMENTATION - FIXED VERSION\n",
      "Object: 01 | Points: 500 | Batch: 8\n",
      "Loading ape (asymmetric)...\n",
      "Found 186 valid samples\n",
      "Loading ape (asymmetric)...\n",
      "Found 1050 valid samples\n",
      "✓ Model parameters: 28,156,682\n",
      "\n",
      "🧪 Testing FIXED forward pass...\n",
      "✅ FIXED forward pass successful!\n",
      "   Rotation shape: torch.Size([1, 3, 3])\n",
      "   Translation shape: torch.Size([1, 3])\n",
      "   Rotation matrix sample:\n",
      "tensor([[-0.1331,  0.9869, -0.0913],\n",
      "        [ 0.4379, -0.0241, -0.8987],\n",
      "        [-0.8891, -0.1596, -0.4289]], device='cuda:0')\n",
      "   Translation sample: tensor([-1.2379, -0.3849, -1.3392], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# EXACT PAPER ARCHITECTURE - FIXED VERSION\n",
    "# ==============================================================================\n",
    "class PositionalEncoding1D(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "class PointCloudPositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(3, d_model//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model//2, d_model)\n",
    "        )\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, point_cloud):\n",
    "        # point_cloud shape: (batch_size, num_points, 3)\n",
    "        batch_size, num_points, _ = point_cloud.shape\n",
    "\n",
    "        # Process each point independently\n",
    "        point_cloud_flat = point_cloud.reshape(-1, 3)  # (batch_size * num_points, 3)\n",
    "        pos_enc_flat = self.mlp(point_cloud_flat)  # (batch_size * num_points, d_model)\n",
    "        pos_enc = pos_enc_flat.reshape(batch_size, num_points, -1)  # (batch_size, num_points, d_model)\n",
    "\n",
    "        return pos_enc\n",
    "\n",
    "class WorkingPFE(nn.Module):\n",
    "    \"\"\"Pixel-wise Feature Extraction - FIXED VERSION\"\"\"\n",
    "    def __init__(self, feature_dim=256, num_layers=4, nhead=8, num_points=500):\n",
    "        super().__init__()\n",
    "        self.num_points = num_points\n",
    "        self.feature_dim = feature_dim\n",
    "\n",
    "        # RGB: CNN + ViT like paper\n",
    "        self.cnn_backbone = timm.create_model('resnet18', pretrained=True, features_only=True)\n",
    "        self.cnn_proj = nn.Conv2d(512, feature_dim, 1)\n",
    "\n",
    "        self.vit_backbone = timm.create_model('vit_tiny_patch16_224', pretrained=True, num_classes=0)\n",
    "        self.vit_proj = nn.Linear(192, feature_dim)\n",
    "\n",
    "        # Point Cloud: MLP like paper - FIXED DIMENSIONS\n",
    "        self.pc_mlp = nn.Sequential(\n",
    "            nn.Linear(3, 64),  # Reduced for stability\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, feature_dim)\n",
    "        )\n",
    "\n",
    "        self.rgb_pos_enc = PositionalEncoding1D(feature_dim)\n",
    "        self.pc_pos_enc = PointCloudPositionalEncoding(feature_dim)\n",
    "\n",
    "        # Transformers with better initialization\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=feature_dim, nhead=nhead, batch_first=True,\n",
    "            dim_feedforward=feature_dim*2, dropout=0.1\n",
    "        )\n",
    "        self.rgb_transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.pc_transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Initialize properly\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, rgb, points):\n",
    "        batch_size = rgb.shape[0]\n",
    "\n",
    "        # RGB Processing\n",
    "        cnn_features = self.cnn_backbone(rgb)[-1]  # (batch, 512, H, W)\n",
    "        cnn_features = self.cnn_proj(cnn_features)  # (batch, feature_dim, H, W)\n",
    "        cnn_features = cnn_features.view(batch_size, self.feature_dim, -1)  # (batch, feature_dim, H*W)\n",
    "        cnn_features = cnn_features.transpose(1, 2)  # (batch, H*W, feature_dim)\n",
    "\n",
    "        vit_features = self.vit_backbone(rgb)  # (batch, 192)\n",
    "        vit_features = self.vit_proj(vit_features)  # (batch, feature_dim)\n",
    "        vit_features = vit_features.unsqueeze(1).expand(-1, cnn_features.shape[1], -1)  # (batch, H*W, feature_dim)\n",
    "\n",
    "        rgb_features = cnn_features + vit_features\n",
    "\n",
    "        # Expand to match points - FIXED LOGIC\n",
    "        current_points = rgb_features.shape[1]\n",
    "        if current_points < self.num_points:\n",
    "            # Repeat features to reach num_points\n",
    "            repeat_factor = (self.num_points // current_points) + 1\n",
    "            rgb_features = rgb_features.repeat(1, repeat_factor, 1)\n",
    "        rgb_features = rgb_features[:, :self.num_points, :]  # (batch, num_points, feature_dim)\n",
    "\n",
    "        rgb_features = self.rgb_pos_enc(rgb_features)\n",
    "        rgb_features = self.rgb_transformer(rgb_features)\n",
    "\n",
    "        # Point Cloud Processing - FIXED\n",
    "        batch_size, num_points, _ = points.shape\n",
    "\n",
    "        # Process point cloud through MLP\n",
    "        points_flat = points.reshape(-1, 3)  # (batch_size * num_points, 3)\n",
    "        pc_features_flat = self.pc_mlp(points_flat)  # (batch_size * num_points, feature_dim)\n",
    "        pc_features = pc_features_flat.reshape(batch_size, num_points, self.feature_dim)  # (batch_size, num_points, feature_dim)\n",
    "\n",
    "        # Add positional encoding using the original point coordinates\n",
    "        pc_pos_enc = self.pc_pos_enc(points)  # (batch_size, num_points, feature_dim)\n",
    "        pc_features = pc_features + pc_pos_enc\n",
    "\n",
    "        pc_features = self.pc_transformer(pc_features)\n",
    "\n",
    "        return rgb_features, pc_features\n",
    "\n",
    "class WorkingMMF(nn.Module):\n",
    "    \"\"\"Multi-Modal Fusion - FIXED VERSION\"\"\"\n",
    "    def __init__(self, feature_dim=256, num_layers=4, nhead=8):\n",
    "        super().__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "\n",
    "        # Adjust nhead if feature_dim*2 is not divisible by nhead\n",
    "        actual_nhead = min(nhead, (feature_dim * 2) // 64)  # Ensure each head has at least 64 dimensions\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=feature_dim*2, nhead=actual_nhead, batch_first=True,\n",
    "            dim_feedforward=feature_dim*4, dropout=0.1\n",
    "        )\n",
    "        self.fusion_transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fusion_pos_enc = PositionalEncoding1D(feature_dim*2)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, rgb_features, pc_features):\n",
    "        fused_features = torch.cat([rgb_features, pc_features], dim=-1)  # (batch, num_points, feature_dim*2)\n",
    "        fused_features = self.fusion_pos_enc(fused_features)\n",
    "        fused_features = self.fusion_transformer(fused_features)\n",
    "        return fused_features\n",
    "\n",
    "class WorkingPosePredictor(nn.Module):\n",
    "    \"\"\"Pose Predictor - FIXED VERSION\"\"\"\n",
    "    def __init__(self, feature_dim=256, num_points=500):\n",
    "        super().__init__()\n",
    "        self.num_points = num_points\n",
    "        self.feature_dim = feature_dim\n",
    "\n",
    "        # Per-point prediction EXACTLY like paper\n",
    "        self.rotation_head = nn.Sequential(\n",
    "            nn.Linear(feature_dim * 2, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 6)  # 6D rotation\n",
    "        )\n",
    "\n",
    "        self.translation_head = nn.Sequential(\n",
    "            nn.Linear(feature_dim * 2, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 3)\n",
    "        )\n",
    "\n",
    "        self.confidence_head = nn.Sequential(\n",
    "            nn.Linear(feature_dim * 2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, fused_features):\n",
    "        batch_size, num_points, feature_dim = fused_features.shape\n",
    "\n",
    "        # Each point predicts pose + confidence - PAPER EXACT\n",
    "        rotations = self.rotation_head(fused_features)  # (batch, num_points, 6)\n",
    "        translations = self.translation_head(fused_features)  # (batch, num_points, 3)\n",
    "        confidences = self.confidence_head(fused_features)  # (batch, num_points, 1)\n",
    "\n",
    "        # Confidence voting - PAPER EXACT\n",
    "        best_idx = torch.argmax(confidences.squeeze(-1), dim=1)  # (batch,)\n",
    "\n",
    "        best_rotations = rotations[torch.arange(batch_size), best_idx]  # (batch, 6)\n",
    "        best_translations = translations[torch.arange(batch_size), best_idx]  # (batch, 3)\n",
    "\n",
    "        return best_rotations, best_translations\n",
    "\n",
    "class WorkingPaperModel(nn.Module):\n",
    "    \"\"\"COMPLETE PAPER MODEL - FIXED ARCHITECTURE\"\"\"\n",
    "    def __init__(self, num_points=500, feature_dim=256, nhead=8, num_layers=4):\n",
    "        super().__init__()\n",
    "        self.pfe = WorkingPFE(feature_dim, num_layers, nhead, num_points)\n",
    "        self.mmf = WorkingMMF(feature_dim, num_layers=3, nhead=nhead)\n",
    "        self.pose_predictor = WorkingPosePredictor(feature_dim, num_points)\n",
    "\n",
    "    def forward(self, rgb, points):\n",
    "        rgb_features, pc_features = self.pfe(rgb, points)\n",
    "        fused_features = self.mmf(rgb_features, pc_features)\n",
    "        rotation_6d, translation = self.pose_predictor(fused_features)\n",
    "\n",
    "        # Convert 6D to rotation matrix - PAPER EXACT\n",
    "        rotation_matrix = self.ortho6d_to_rotation_matrix(rotation_6d)\n",
    "        return rotation_matrix, translation\n",
    "\n",
    "    def ortho6d_to_rotation_matrix(self, ortho6d):\n",
    "        x = ortho6d[:, 0:3]\n",
    "        y = ortho6d[:, 3:6]\n",
    "        x = F.normalize(x, p=2, dim=1)\n",
    "        z = torch.cross(x, y, dim=1)\n",
    "        z = F.normalize(z, p=2, dim=1)\n",
    "        y = torch.cross(z, x, dim=1)\n",
    "        rotation_matrix = torch.stack([x, y, z], dim=2)\n",
    "        return rotation_matrix\n",
    "\n",
    "# ==============================================================================\n",
    "# TEST THE FIXED MODEL\n",
    "# ==============================================================================\n",
    "if __name__ == '__main__':\n",
    "    print(f\"\\n🎯 EXACT PAPER IMPLEMENTATION - FIXED VERSION\")\n",
    "    print(f\"Object: {OBJECT_ID_STR} | Points: {NUM_POINTS} | Batch: {BATCH_SIZE}\")\n",
    "\n",
    "    # Load datasets\n",
    "    train_dataset = ComprehensiveLinemodDataset(base_dir, OBJECT_ID_STR, is_train=True, num_points=NUM_POINTS)\n",
    "    test_dataset = ComprehensiveLinemodDataset(base_dir, OBJECT_ID_STR, is_train=False, num_points=NUM_POINTS)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "    # Initialize FIXED PAPER model\n",
    "    model = WorkingPaperModel(\n",
    "        num_points=NUM_POINTS,\n",
    "        feature_dim=FEATURE_DIM,\n",
    "        nhead=NHEAD,\n",
    "        num_layers=NUM_LAYERS\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    print(f\"✓ Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "    # Test forward pass with debugging\n",
    "    print(\"\\n🧪 Testing FIXED forward pass...\")\n",
    "    test_batch = next(iter(train_loader))\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            pred_r, pred_t = model(test_batch['rgb'][:1].to(DEVICE), test_batch['points'][:1].to(DEVICE))\n",
    "        print(\"✅ FIXED forward pass successful!\")\n",
    "        print(f\"   Rotation shape: {pred_r.shape}\")  # Should be (1, 3, 3)\n",
    "        print(f\"   Translation shape: {pred_t.shape}\")  # Should be (1, 3)\n",
    "        print(f\"   Rotation matrix sample:\\n{pred_r[0]}\")\n",
    "        print(f\"   Translation sample: {pred_t[0]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Forward pass still failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VRBoJi7N4Gx3"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UYKll_wCuecA",
    "outputId": "fefa3092-5e19-4f3b-9a03-43d12c0f12f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 EXACT PAPER IMPLEMENTATION - COMPLETE WORKING SCRIPT\n",
      "🎯 PAPER EXACT IMPLEMENTATION | Device: cuda\n",
      "\n",
      "🎯 EXACT PAPER IMPLEMENTATION - COMPLETE WORKING SCRIPT\n",
      "Object: 01 | Points: 500 | Batch: 8\n",
      "Epochs: 200 | LR: 0.0001\n",
      "✓ Multi-modal features (RGB + Point Cloud)\n",
      "✓ Transformer fusion\n",
      "✓ 6D rotation representation\n",
      "✓ Per-point confidence voting\n",
      "✓ PAPER EXACT ARCHITECTURE\n",
      "\n",
      "Found 186 valid samples\n",
      "Found 1050 valid samples\n",
      "✓ Training: 186 samples\n",
      "✓ Testing: 1050 samples\n",
      "\n",
      "📊 Object Info:\n",
      "  Name: ape\n",
      "  Diameter: 0.102m\n",
      "  Symmetric: False\n",
      "\n",
      "🧪 Testing paper forward pass...\n",
      "✅ Paper forward pass successful!\n",
      "   Model parameters: 28,156,682\n",
      "   Rotation shape: torch.Size([1, 3, 3]), Translation shape: torch.Size([1, 3])\n",
      "\n",
      "🔧 STABILITY IMPROVEMENTS:\n",
      "   • Proper weight initialization\n",
      "   • Gradient clipping\n",
      "   • Learning rate warmup\n",
      "   • Batch normalization in MLP\n",
      "   • Smaller learning rate (1e-4)\n",
      "\n",
      "🚀 STARTING PAPER TRAINING\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-3087581995.py:460: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  return float(np.trapz(accuracies, thresholds) / max_threshold * 100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 Epoch 01/200 - Results:\n",
      "   Train Loss: 0.7149 | LR: 1.00e-05\n",
      "   ADD(S) Mean: 0.2482m\n",
      "   Rotation Error: 128.67°\n",
      "   Translation Error: 0.2423m\n",
      "   Accuracy @5cm: 0.29%\n",
      "   Accuracy @10cm: 8.29%\n",
      "   AUC: 2.07%\n",
      "   🎯 NEW BEST! Accuracy: 0.29%\n",
      "   ⏱️  Epoch Time: 1.3min | Total: 1.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 Epoch 06/200 - Results:\n",
      "   Train Loss: 0.2661 | LR: 6.00e-05\n",
      "   ADD(S) Mean: 0.2592m\n",
      "   Rotation Error: 97.83°\n",
      "   Translation Error: 0.2564m\n",
      "   Accuracy @5cm: 0.00%\n",
      "   Accuracy @10cm: 0.00%\n",
      "   AUC: 0.00%\n",
      "   No improvement (1/30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 Epoch 11/200 - Results:\n",
      "   Train Loss: 0.2295 | LR: 9.98e-05\n",
      "   ADD(S) Mean: 0.1137m\n",
      "   Rotation Error: 96.84°\n",
      "   Translation Error: 0.1068m\n",
      "   Accuracy @5cm: 5.71%\n",
      "   Accuracy @10cm: 46.57%\n",
      "   AUC: 12.94%\n",
      "   🎯 NEW BEST! Accuracy: 5.71%\n",
      "   ⏱️  Epoch Time: 1.3min | Total: 6.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 Epoch 16/200 - Results:\n",
      "   Train Loss: 0.1682 | LR: 9.89e-05\n",
      "   ADD(S) Mean: 0.0956m\n",
      "   Rotation Error: 94.98°\n",
      "   Translation Error: 0.0879m\n",
      "   Accuracy @5cm: 11.62%\n",
      "   Accuracy @10cm: 65.33%\n",
      "   AUC: 20.80%\n",
      "   🎯 NEW BEST! Accuracy: 11.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 Epoch 21/200 - Results:\n",
      "   Train Loss: 0.1565 | LR: 9.78e-05\n",
      "   ADD(S) Mean: 0.0951m\n",
      "   Rotation Error: 93.77°\n",
      "   Translation Error: 0.0877m\n",
      "   Accuracy @5cm: 10.57%\n",
      "   Accuracy @10cm: 58.00%\n",
      "   AUC: 18.18%\n",
      "   No improvement (1/30)\n",
      "   ⏱️  Epoch Time: 1.3min | Total: 10.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 Epoch 26/200 - Results:\n",
      "   Train Loss: 0.1449 | LR: 9.64e-05\n",
      "   ADD(S) Mean: 0.0904m\n",
      "   Rotation Error: 94.76°\n",
      "   Translation Error: 0.0823m\n",
      "   Accuracy @5cm: 14.67%\n",
      "   Accuracy @10cm: 64.48%\n",
      "   AUC: 22.11%\n",
      "   🎯 NEW BEST! Accuracy: 14.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 Epoch 31/200 - Results:\n",
      "   Train Loss: 0.1402 | LR: 9.47e-05\n",
      "   ADD(S) Mean: 0.0912m\n",
      "   Rotation Error: 92.35°\n",
      "   Translation Error: 0.0852m\n",
      "   Accuracy @5cm: 11.81%\n",
      "   Accuracy @10cm: 62.48%\n",
      "   AUC: 20.26%\n",
      "   No improvement (1/30)\n",
      "   ⏱️  Epoch Time: 1.3min | Total: 15.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 Epoch 36/200 - Results:\n",
      "   Train Loss: 0.1267 | LR: 9.27e-05\n",
      "   ADD(S) Mean: 0.0718m\n",
      "   Rotation Error: 92.22°\n",
      "   Translation Error: 0.0616m\n",
      "   Accuracy @5cm: 17.90%\n",
      "   Accuracy @10cm: 85.81%\n",
      "   AUC: 30.53%\n",
      "   🎯 NEW BEST! Accuracy: 17.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 Epoch 41/200 - Results:\n",
      "   Train Loss: 0.1240 | LR: 9.04e-05\n",
      "   ADD(S) Mean: 0.1071m\n",
      "   Rotation Error: 90.00°\n",
      "   Translation Error: 0.1024m\n",
      "   Accuracy @5cm: 1.43%\n",
      "   Accuracy @10cm: 44.38%\n",
      "   AUC: 9.48%\n",
      "   No improvement (1/30)\n",
      "   ⏱️  Epoch Time: 1.3min | Total: 20.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 Epoch 46/200 - Results:\n",
      "   Train Loss: 0.1275 | LR: 8.79e-05\n",
      "   ADD(S) Mean: 0.0930m\n",
      "   Rotation Error: 89.40°\n",
      "   Translation Error: 0.0872m\n",
      "   Accuracy @5cm: 6.00%\n",
      "   Accuracy @10cm: 62.95%\n",
      "   AUC: 17.37%\n",
      "   No improvement (2/30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 Epoch 51/200 - Results:\n",
      "   Train Loss: 0.1199 | LR: 8.52e-05\n",
      "   ADD(S) Mean: 0.0845m\n",
      "   Rotation Error: 87.06°\n",
      "   Translation Error: 0.0778m\n",
      "   Accuracy @5cm: 7.62%\n",
      "   Accuracy @10cm: 76.19%\n",
      "   AUC: 20.63%\n",
      "   No improvement (3/30)\n",
      "   ⏱️  Epoch Time: 1.3min | Total: 24.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 Epoch 56/200 - Results:\n",
      "   Train Loss: 0.1140 | LR: 8.23e-05\n",
      "   ADD(S) Mean: 0.1112m\n",
      "   Rotation Error: 86.30°\n",
      "   Translation Error: 0.1068m\n",
      "   Accuracy @5cm: 2.10%\n",
      "   Accuracy @10cm: 36.76%\n",
      "   AUC: 7.19%\n",
      "   No improvement (4/30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 Epoch 61/200 - Results:\n",
      "   Train Loss: 0.1107 | LR: 7.91e-05\n",
      "   ADD(S) Mean: 0.0806m\n",
      "   Rotation Error: 84.57°\n",
      "   Translation Error: 0.0738m\n",
      "   Accuracy @5cm: 10.00%\n",
      "   Accuracy @10cm: 80.86%\n",
      "   AUC: 24.92%\n",
      "   No improvement (5/30)\n",
      "   ⏱️  Epoch Time: 1.3min | Total: 29.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 Epoch 66/200 - Results:\n",
      "   Train Loss: 0.1022 | LR: 7.58e-05\n",
      "   ADD(S) Mean: 0.0992m\n",
      "   Rotation Error: 83.33°\n",
      "   Translation Error: 0.0936m\n",
      "   Accuracy @5cm: 5.71%\n",
      "   Accuracy @10cm: 51.43%\n",
      "   AUC: 12.98%\n",
      "   No improvement (6/30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 Epoch 71/200 - Results:\n",
      "   Train Loss: 0.1073 | LR: 7.24e-05\n",
      "   ADD(S) Mean: 0.0781m\n",
      "   Rotation Error: 83.27°\n",
      "   Translation Error: 0.0715m\n",
      "   Accuracy @5cm: 10.48%\n",
      "   Accuracy @10cm: 84.00%\n",
      "   AUC: 24.17%\n",
      "   No improvement (7/30)\n",
      "   ⏱️  Epoch Time: 1.3min | Total: 33.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 Epoch 76/200 - Results:\n",
      "   Train Loss: 0.1125 | LR: 6.87e-05\n",
      "   ADD(S) Mean: 0.0970m\n",
      "   Rotation Error: 82.85°\n",
      "   Translation Error: 0.0922m\n",
      "   Accuracy @5cm: 2.67%\n",
      "   Accuracy @10cm: 57.24%\n",
      "   AUC: 13.82%\n",
      "   No improvement (8/30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 Epoch 81/200 - Results:\n",
      "   Train Loss: 0.0985 | LR: 6.50e-05\n",
      "   ADD(S) Mean: 0.0666m\n",
      "   Rotation Error: 81.31°\n",
      "   Translation Error: 0.0592m\n",
      "   Accuracy @5cm: 20.57%\n",
      "   Accuracy @10cm: 90.95%\n",
      "   AUC: 35.17%\n",
      "   🎯 NEW BEST! Accuracy: 20.57%\n",
      "   ⏱️  Epoch Time: 1.3min | Total: 38.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 Epoch 86/200 - Results:\n",
      "   Train Loss: 0.0986 | LR: 6.12e-05\n",
      "   ADD(S) Mean: 0.0905m\n",
      "   Rotation Error: 80.42°\n",
      "   Translation Error: 0.0846m\n",
      "   Accuracy @5cm: 7.05%\n",
      "   Accuracy @10cm: 65.52%\n",
      "   AUC: 17.73%\n",
      "   No improvement (1/30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 Epoch 91/200 - Results:\n",
      "   Train Loss: 0.0966 | LR: 5.73e-05\n",
      "   ADD(S) Mean: 0.0829m\n",
      "   Rotation Error: 81.57°\n",
      "   Translation Error: 0.0748m\n",
      "   Accuracy @5cm: 19.24%\n",
      "   Accuracy @10cm: 71.33%\n",
      "   AUC: 26.51%\n",
      "   No improvement (2/30)\n",
      "   ⏱️  Epoch Time: 1.3min | Total: 43.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 Epoch 96/200 - Results:\n",
      "   Train Loss: 0.0906 | LR: 5.34e-05\n",
      "   ADD(S) Mean: 0.0715m\n",
      "   Rotation Error: 81.54°\n",
      "   Translation Error: 0.0628m\n",
      "   Accuracy @5cm: 25.62%\n",
      "   Accuracy @10cm: 83.05%\n",
      "   AUC: 32.13%\n",
      "   🎯 NEW BEST! Accuracy: 25.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 Epoch 101/200 - Results:\n",
      "   Train Loss: 0.1011 | LR: 4.95e-05\n",
      "   ADD(S) Mean: 0.0968m\n",
      "   Rotation Error: 84.43°\n",
      "   Translation Error: 0.0910m\n",
      "   Accuracy @5cm: 1.33%\n",
      "   Accuracy @10cm: 60.10%\n",
      "   AUC: 11.81%\n",
      "   No improvement (1/30)\n",
      "   ⏱️  Epoch Time: 1.3min | Total: 48.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 Epoch 106/200 - Results:\n",
      "   Train Loss: 0.0885 | LR: 4.55e-05\n",
      "   ADD(S) Mean: 0.0907m\n",
      "   Rotation Error: 79.69°\n",
      "   Translation Error: 0.0850m\n",
      "   Accuracy @5cm: 3.52%\n",
      "   Accuracy @10cm: 66.29%\n",
      "   AUC: 16.34%\n",
      "   No improvement (2/30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 Epoch 111/200 - Results:\n",
      "   Train Loss: 0.0922 | LR: 4.16e-05\n",
      "   ADD(S) Mean: 0.0913m\n",
      "   Rotation Error: 80.35°\n",
      "   Translation Error: 0.0855m\n",
      "   Accuracy @5cm: 2.10%\n",
      "   Accuracy @10cm: 68.67%\n",
      "   AUC: 14.44%\n",
      "   No improvement (3/30)\n",
      "   ⏱️  Epoch Time: 1.3min | Total: 52.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 Epoch 116/200 - Results:\n",
      "   Train Loss: 0.0924 | LR: 3.78e-05\n",
      "   ADD(S) Mean: 0.0691m\n",
      "   Rotation Error: 80.13°\n",
      "   Translation Error: 0.0617m\n",
      "   Accuracy @5cm: 18.38%\n",
      "   Accuracy @10cm: 90.29%\n",
      "   AUC: 32.03%\n",
      "   No improvement (4/30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 Epoch 121/200 - Results:\n",
      "   Train Loss: 0.0860 | LR: 3.40e-05\n",
      "   ADD(S) Mean: 0.0986m\n",
      "   Rotation Error: 78.89°\n",
      "   Translation Error: 0.0934m\n",
      "   Accuracy @5cm: 1.52%\n",
      "   Accuracy @10cm: 57.62%\n",
      "   AUC: 10.63%\n",
      "   No improvement (5/30)\n",
      "   ⏱️  Epoch Time: 1.3min | Total: 57.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 Epoch 126/200 - Results:\n",
      "   Train Loss: 0.0820 | LR: 3.03e-05\n",
      "   ADD(S) Mean: 0.0804m\n",
      "   Rotation Error: 79.60°\n",
      "   Translation Error: 0.0743m\n",
      "   Accuracy @5cm: 8.86%\n",
      "   Accuracy @10cm: 78.38%\n",
      "   AUC: 23.20%\n",
      "   No improvement (6/30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 Epoch 131/200 - Results:\n",
      "   Train Loss: 0.0828 | LR: 2.67e-05\n",
      "   ADD(S) Mean: 0.0865m\n",
      "   Rotation Error: 78.83°\n",
      "   Translation Error: 0.0806m\n",
      "   Accuracy @5cm: 5.43%\n",
      "   Accuracy @10cm: 74.57%\n",
      "   AUC: 17.82%\n",
      "   No improvement (7/30)\n",
      "   ⏱️  Epoch Time: 1.3min | Total: 61.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 Epoch 136/200 - Results:\n",
      "   Train Loss: 0.0865 | LR: 2.33e-05\n",
      "   ADD(S) Mean: 0.0787m\n",
      "   Rotation Error: 79.78°\n",
      "   Translation Error: 0.0720m\n",
      "   Accuracy @5cm: 10.76%\n",
      "   Accuracy @10cm: 83.24%\n",
      "   AUC: 23.38%\n",
      "   No improvement (8/30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 139:  50%|█████     | 12/24 [00:07<00:07,  1.50it/s, loss=0.0828]"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# EXACT PAPER IMPLEMENTATION - COMPLETE COPY & PASTE SCRIPT\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"🎯 EXACT PAPER IMPLEMENTATION - COMPLETE WORKING SCRIPT\")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import cv2\n",
    "import yaml\n",
    "import os\n",
    "import open3d as o3d\n",
    "import time\n",
    "import math\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import timm\n",
    "\n",
    "# ==============================================================================\n",
    "# CONFIGURATION - PAPER EXACT\n",
    "# ==============================================================================\n",
    "project_dir = '/content/drive/My Drive/Project'\n",
    "base_dir = os.path.join(project_dir, 'Linemod_preprocessed')\n",
    "\n",
    "OBJECT_ID_STR = '01'\n",
    "NUM_POINTS = 500\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 200\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "# PAPER EXACT DIMENSIONS\n",
    "FEATURE_DIM = 256\n",
    "NHEAD = 8\n",
    "NUM_LAYERS = 4\n",
    "\n",
    "SYMMETRIC_OBJECTS = {'eggbox', 'glue'}\n",
    "OBJECT_NAMES = {\n",
    "    '01': 'ape', '02': 'benchvise', '03': 'camera', '04': 'can',\n",
    "    '05': 'cat', '06': 'driller', '07': 'duck', '08': 'eggbox',\n",
    "    '09': 'glue', '10': 'holepuncher', '11': 'iron', '12': 'lamp',\n",
    "    '13': 'phone'\n",
    "}\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"🎯 PAPER EXACT IMPLEMENTATION | Device: {DEVICE}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# DATASET CLASS - KEEPING YOUR EXACT PATHS\n",
    "# ==============================================================================\n",
    "class SafeDataset(Dataset):\n",
    "    def __init__(self, root_dir, object_id_str, is_train=True, num_points=500):\n",
    "        self.root_dir = root_dir\n",
    "        self.object_id_str = object_id_str\n",
    "        self.object_id_int = int(object_id_str)\n",
    "        self.num_points = num_points\n",
    "        self.is_train = is_train\n",
    "\n",
    "        data_folder_root = os.path.join(self.root_dir, 'data')\n",
    "        object_data_path = os.path.join(data_folder_root, self.object_id_str)\n",
    "\n",
    "        list_file = os.path.join(object_data_path, 'train.txt' if is_train else 'test.txt')\n",
    "        with open(list_file) as f:\n",
    "            self.file_list = [line.strip() for line in f.readlines()]\n",
    "\n",
    "        self.rgb_dir = os.path.join(object_data_path, 'rgb')\n",
    "        self.depth_dir = os.path.join(object_data_path, 'depth')\n",
    "        self.mask_dir = os.path.join(object_data_path, 'mask')\n",
    "\n",
    "        gt_file = os.path.join(object_data_path, 'gt.yml')\n",
    "        info_file = os.path.join(object_data_path, 'info.yml')\n",
    "\n",
    "        with open(gt_file, 'r') as f:\n",
    "            self.gt_data = yaml.safe_load(f)\n",
    "        with open(info_file, 'r') as f:\n",
    "            self.info_data = yaml.safe_load(f)\n",
    "\n",
    "        model_file = os.path.join(self.root_dir, 'models', f'obj_{object_id_str}.ply')\n",
    "        self.model_points = np.asarray(o3d.io.read_point_cloud(model_file).points) / 1000.0\n",
    "\n",
    "        self.rgb_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "        self.valid_indices = self._precompute_valid_samples()\n",
    "        print(f\"Found {len(self.valid_indices)} valid samples\")\n",
    "\n",
    "    def _precompute_valid_samples(self):\n",
    "        valid_indices = []\n",
    "        for idx in range(len(self.file_list)):\n",
    "            try:\n",
    "                frame_idx = int(self.file_list[idx])\n",
    "                if frame_idx not in self.gt_data or frame_idx not in self.info_data:\n",
    "                    continue\n",
    "                found_object = False\n",
    "                for obj_gt in self.gt_data[frame_idx]:\n",
    "                    if obj_gt['obj_id'] == self.object_id_int:\n",
    "                        found_object = True\n",
    "                        break\n",
    "                if not found_object:\n",
    "                    continue\n",
    "                valid_indices.append(idx)\n",
    "            except:\n",
    "                continue\n",
    "        return valid_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        actual_idx = self.valid_indices[idx]\n",
    "        frame_idx = int(self.file_list[actual_idx])\n",
    "\n",
    "        cam_k = np.array(self.info_data[frame_idx]['cam_K']).reshape(3, 3)\n",
    "        fx, fy, cx, cy = cam_k[0, 0], cam_k[1, 1], cam_k[0, 2], cam_k[1, 2]\n",
    "        depth_scale = self.info_data[frame_idx]['depth_scale']\n",
    "\n",
    "        gt_rotation, gt_translation = None, None\n",
    "        for obj_gt in self.gt_data[frame_idx]:\n",
    "            if obj_gt['obj_id'] == self.object_id_int:\n",
    "                gt_rotation = np.array(obj_gt['cam_R_m2c']).reshape(3, 3)\n",
    "                gt_translation = np.array(obj_gt['cam_t_m2c']) / 1000.0\n",
    "                break\n",
    "\n",
    "        rgb_img = cv2.imread(os.path.join(self.rgb_dir, f'{self.file_list[actual_idx]}.png'))\n",
    "        rgb_img = cv2.cvtColor(rgb_img, cv2.COLOR_BGR2RGB)\n",
    "        depth_img = cv2.imread(os.path.join(self.depth_dir, f'{self.file_list[actual_idx]}.png'), cv2.IMREAD_UNCHANGED)\n",
    "        mask = cv2.imread(os.path.join(self.mask_dir, f'{self.file_list[actual_idx]}.png'), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        indices = np.where(mask > 0)\n",
    "        points = []\n",
    "        for i in range(0, len(indices[0]), 2):  # Sparse sampling for speed\n",
    "            v, u = indices[0][i], indices[1][i]\n",
    "            d = depth_img[v, u] * depth_scale / 1000.0\n",
    "            if d > 0:\n",
    "                points.append([(u - cx) * d / fx, (v - cy) * d / fy, d])\n",
    "\n",
    "        points_np = np.array(points)\n",
    "        if len(points_np) < 5:\n",
    "            points_np = (np.random.rand(self.num_points, 3) - 0.5) * 0.2\n",
    "\n",
    "        if len(points_np) > self.num_points:\n",
    "            sample_indices = np.random.choice(len(points_np), self.num_points, replace=False)\n",
    "        else:\n",
    "            sample_indices = np.random.choice(len(points_np), self.num_points, replace=True)\n",
    "\n",
    "        points_tensor = torch.from_numpy(points_np[sample_indices]).float()\n",
    "        rgb_tensor = self.rgb_transform(rgb_img)\n",
    "\n",
    "        return {\n",
    "            'rgb': rgb_tensor,\n",
    "            'points': points_tensor,\n",
    "            'gt_rotation': torch.from_numpy(gt_rotation).float(),\n",
    "            'gt_translation': torch.from_numpy(gt_translation).float(),\n",
    "        }\n",
    "\n",
    "# ==============================================================================\n",
    "# EXACT PAPER ARCHITECTURE\n",
    "# ==============================================================================\n",
    "class PositionalEncoding1D(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "class PointCloudPositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(3, d_model//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model//2, d_model)\n",
    "        )\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, point_cloud):\n",
    "        batch_size, num_points, _ = point_cloud.shape\n",
    "        point_cloud_flat = point_cloud.reshape(-1, 3)\n",
    "        pos_enc_flat = self.mlp(point_cloud_flat)\n",
    "        pos_enc = pos_enc_flat.reshape(batch_size, num_points, -1)\n",
    "        return pos_enc\n",
    "\n",
    "class WorkingPFE(nn.Module):\n",
    "    def __init__(self, feature_dim=256, num_layers=4, nhead=8, num_points=500):\n",
    "        super().__init__()\n",
    "        self.num_points = num_points\n",
    "        self.feature_dim = feature_dim\n",
    "\n",
    "        # RGB: CNN + ViT like paper\n",
    "        self.cnn_backbone = timm.create_model('resnet18', pretrained=True, features_only=True)\n",
    "        self.cnn_proj = nn.Conv2d(512, feature_dim, 1)\n",
    "\n",
    "        self.vit_backbone = timm.create_model('vit_tiny_patch16_224', pretrained=True, num_classes=0)\n",
    "        self.vit_proj = nn.Linear(192, feature_dim)\n",
    "\n",
    "        # Point Cloud: MLP like paper\n",
    "        self.pc_mlp = nn.Sequential(\n",
    "            nn.Linear(3, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, feature_dim)\n",
    "        )\n",
    "\n",
    "        self.rgb_pos_enc = PositionalEncoding1D(feature_dim)\n",
    "        self.pc_pos_enc = PointCloudPositionalEncoding(feature_dim)\n",
    "\n",
    "        # Transformers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=feature_dim, nhead=nhead, batch_first=True,\n",
    "            dim_feedforward=feature_dim*2, dropout=0.1\n",
    "        )\n",
    "        self.rgb_transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.pc_transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, rgb, points):\n",
    "        batch_size = rgb.shape[0]\n",
    "\n",
    "        # RGB Processing\n",
    "        cnn_features = self.cnn_backbone(rgb)[-1]\n",
    "        cnn_features = self.cnn_proj(cnn_features)\n",
    "        cnn_features = cnn_features.view(batch_size, self.feature_dim, -1)\n",
    "        cnn_features = cnn_features.transpose(1, 2)\n",
    "\n",
    "        vit_features = self.vit_backbone(rgb)\n",
    "        vit_features = self.vit_proj(vit_features)\n",
    "        vit_features = vit_features.unsqueeze(1).expand(-1, cnn_features.shape[1], -1)\n",
    "\n",
    "        rgb_features = cnn_features + vit_features\n",
    "\n",
    "        # Expand to match points\n",
    "        current_points = rgb_features.shape[1]\n",
    "        if current_points < self.num_points:\n",
    "            repeat_factor = (self.num_points // current_points) + 1\n",
    "            rgb_features = rgb_features.repeat(1, repeat_factor, 1)\n",
    "        rgb_features = rgb_features[:, :self.num_points, :]\n",
    "\n",
    "        rgb_features = self.rgb_pos_enc(rgb_features)\n",
    "        rgb_features = self.rgb_transformer(rgb_features)\n",
    "\n",
    "        # Point Cloud Processing\n",
    "        batch_size, num_points, _ = points.shape\n",
    "        points_flat = points.reshape(-1, 3)\n",
    "        pc_features_flat = self.pc_mlp(points_flat)\n",
    "        pc_features = pc_features_flat.reshape(batch_size, num_points, self.feature_dim)\n",
    "\n",
    "        pc_pos_enc = self.pc_pos_enc(points)\n",
    "        pc_features = pc_features + pc_pos_enc\n",
    "        pc_features = self.pc_transformer(pc_features)\n",
    "\n",
    "        return rgb_features, pc_features\n",
    "\n",
    "class WorkingMMF(nn.Module):\n",
    "    def __init__(self, feature_dim=256, num_layers=4, nhead=8):\n",
    "        super().__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "        actual_nhead = min(nhead, (feature_dim * 2) // 64)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=feature_dim*2, nhead=actual_nhead, batch_first=True,\n",
    "            dim_feedforward=feature_dim*4, dropout=0.1\n",
    "        )\n",
    "        self.fusion_transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fusion_pos_enc = PositionalEncoding1D(feature_dim*2)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, rgb_features, pc_features):\n",
    "        fused_features = torch.cat([rgb_features, pc_features], dim=-1)\n",
    "        fused_features = self.fusion_pos_enc(fused_features)\n",
    "        fused_features = self.fusion_transformer(fused_features)\n",
    "        return fused_features\n",
    "\n",
    "class WorkingPosePredictor(nn.Module):\n",
    "    def __init__(self, feature_dim=256, num_points=500):\n",
    "        super().__init__()\n",
    "        self.num_points = num_points\n",
    "        self.feature_dim = feature_dim\n",
    "\n",
    "        self.rotation_head = nn.Sequential(\n",
    "            nn.Linear(feature_dim * 2, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 6)\n",
    "        )\n",
    "\n",
    "        self.translation_head = nn.Sequential(\n",
    "            nn.Linear(feature_dim * 2, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 3)\n",
    "        )\n",
    "\n",
    "        self.confidence_head = nn.Sequential(\n",
    "            nn.Linear(feature_dim * 2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, fused_features):\n",
    "        batch_size, num_points, feature_dim = fused_features.shape\n",
    "\n",
    "        rotations = self.rotation_head(fused_features)\n",
    "        translations = self.translation_head(fused_features)\n",
    "        confidences = self.confidence_head(fused_features)\n",
    "\n",
    "        best_idx = torch.argmax(confidences.squeeze(-1), dim=1)\n",
    "        best_rotations = rotations[torch.arange(batch_size), best_idx]\n",
    "        best_translations = translations[torch.arange(batch_size), best_idx]\n",
    "\n",
    "        return best_rotations, best_translations\n",
    "\n",
    "class WorkingPaperModel(nn.Module):\n",
    "    def __init__(self, num_points=500, feature_dim=256, nhead=8, num_layers=4):\n",
    "        super().__init__()\n",
    "        self.pfe = WorkingPFE(feature_dim, num_layers, nhead, num_points)\n",
    "        self.mmf = WorkingMMF(feature_dim, num_layers=3, nhead=nhead)\n",
    "        self.pose_predictor = WorkingPosePredictor(feature_dim, num_points)\n",
    "\n",
    "    def forward(self, rgb, points):\n",
    "        rgb_features, pc_features = self.pfe(rgb, points)\n",
    "        fused_features = self.mmf(rgb_features, pc_features)\n",
    "        rotation_6d, translation = self.pose_predictor(fused_features)\n",
    "        rotation_matrix = self.ortho6d_to_rotation_matrix(rotation_6d)\n",
    "        return rotation_matrix, translation\n",
    "\n",
    "    def ortho6d_to_rotation_matrix(self, ortho6d):\n",
    "        x = ortho6d[:, 0:3]\n",
    "        y = ortho6d[:, 3:6]\n",
    "        x = F.normalize(x, p=2, dim=1)\n",
    "        z = torch.cross(x, y, dim=1)\n",
    "        z = F.normalize(z, p=2, dim=1)\n",
    "        y = torch.cross(z, x, dim=1)\n",
    "        rotation_matrix = torch.stack([x, y, z], dim=2)\n",
    "        return rotation_matrix\n",
    "\n",
    "# ==============================================================================\n",
    "# TRAINING FUNCTIONS\n",
    "# ==============================================================================\n",
    "def stable_pose_loss(pred_r, pred_t, gt_r, gt_t, model_points, symmetric=False):\n",
    "    pred_pts = torch.matmul(model_points, pred_r.transpose(1, 2)) + pred_t.unsqueeze(1)\n",
    "    gt_pts = torch.matmul(model_points, gt_r.transpose(1, 2)) + gt_t.unsqueeze(1)\n",
    "\n",
    "    if symmetric:\n",
    "        dists = torch.cdist(pred_pts, gt_pts)\n",
    "        min_dists = torch.min(dists, dim=2)[0]\n",
    "        add_loss = torch.mean(min_dists)\n",
    "    else:\n",
    "        add_loss = torch.mean(torch.norm(pred_pts - gt_pts, dim=2))\n",
    "\n",
    "    return add_loss\n",
    "\n",
    "def working_train_epoch(model, loader, optimizer, model_points, device, object_name, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    is_symmetric = object_name in SYMMETRIC_OBJECTS\n",
    "\n",
    "    if epoch < 10:\n",
    "        warmup_factor = (epoch + 1) / 10\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = LEARNING_RATE * warmup_factor\n",
    "\n",
    "    progress_bar = tqdm(loader, desc=f\"Epoch {epoch+1}\", leave=False)\n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred_r, pred_t = model(batch['rgb'].to(device), batch['points'].to(device))\n",
    "\n",
    "        loss = stable_pose_loss(\n",
    "            pred_r, pred_t,\n",
    "            batch['gt_rotation'].to(device),\n",
    "            batch['gt_translation'].to(device),\n",
    "            model_points,\n",
    "            symmetric=is_symmetric\n",
    "        )\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# ==============================================================================\n",
    "# EVALUATION FUNCTIONS\n",
    "# ==============================================================================\n",
    "def calculate_pose_errors(pred_r, pred_t, gt_r, gt_t, model_points, symmetric=False):\n",
    "    pred_pts = torch.matmul(model_points, pred_r.transpose(1, 2)) + pred_t.unsqueeze(1)\n",
    "    gt_pts = torch.matmul(model_points, gt_r.transpose(1, 2)) + gt_t.unsqueeze(1)\n",
    "\n",
    "    if symmetric:\n",
    "        dists = torch.cdist(pred_pts, gt_pts)\n",
    "        errors = torch.mean(torch.min(dists, dim=2)[0], dim=1)\n",
    "    else:\n",
    "        errors = torch.mean(torch.norm(pred_pts - gt_pts, dim=2), dim=1)\n",
    "\n",
    "    return errors.cpu().numpy()\n",
    "\n",
    "def compute_auc(errors, max_threshold=0.1, n_samples=100):\n",
    "    thresholds = np.linspace(0, max_threshold, n_samples)\n",
    "    accuracies = [np.mean(errors < t) for t in thresholds]\n",
    "    return float(np.trapz(accuracies, thresholds) / max_threshold * 100)\n",
    "\n",
    "def comprehensive_evaluation(model, loader, model_points, device, object_name):\n",
    "    model.eval()\n",
    "    is_symmetric = object_name in SYMMETRIC_OBJECTS\n",
    "\n",
    "    all_errors = []\n",
    "    rotation_errors = []\n",
    "    translation_errors = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=f\"Evaluating {object_name}\", leave=False):\n",
    "            pred_r, pred_t = model(batch['rgb'].to(device), batch['points'].to(device))\n",
    "            gt_r = batch['gt_rotation'].to(device)\n",
    "            gt_t = batch['gt_translation'].to(device)\n",
    "\n",
    "            errors = calculate_pose_errors(pred_r, pred_t, gt_r, gt_t, model_points, symmetric=is_symmetric)\n",
    "            all_errors.extend(errors)\n",
    "\n",
    "            rot_diff = torch.bmm(pred_r, gt_r.transpose(1, 2))\n",
    "            trace = torch.diagonal(rot_diff, dim1=-2, dim2=-1).sum(-1)\n",
    "            rotation_error = torch.acos(torch.clamp((trace - 1) / 2, -1 + 1e-6, 1 - 1e-6)) * 180 / math.pi\n",
    "            rotation_errors.extend(rotation_error.cpu().numpy())\n",
    "\n",
    "            trans_error = torch.norm(pred_t - gt_t, dim=1)\n",
    "            translation_errors.extend(trans_error.cpu().numpy())\n",
    "\n",
    "    all_errors = np.array(all_errors)\n",
    "    rotation_errors = np.array(rotation_errors)\n",
    "    translation_errors = np.array(translation_errors)\n",
    "\n",
    "    metrics = {\n",
    "        'object': object_name,\n",
    "        'symmetric': is_symmetric,\n",
    "        'ADD(S)-Mean': float(np.mean(all_errors)),\n",
    "        'ADD(S)-Median': float(np.median(all_errors)),\n",
    "        'ADD(S)-Std': float(np.std(all_errors)),\n",
    "        'Rotation-Error-Mean': float(np.mean(rotation_errors)),\n",
    "        'Translation-Error-Mean': float(np.mean(translation_errors)),\n",
    "        'AUC': compute_auc(all_errors, max_threshold=0.1),\n",
    "        'n_samples': len(all_errors)\n",
    "    }\n",
    "\n",
    "    thresholds = [0.02, 0.05, 0.10]\n",
    "    for threshold in thresholds:\n",
    "        metrics[f'ACC-{int(threshold*100)}cm'] = float(np.mean(all_errors < threshold) * 100)\n",
    "\n",
    "    return metrics, all_errors\n",
    "\n",
    "# ==============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ==============================================================================\n",
    "if __name__ == '__main__':\n",
    "    print(f\"\\n🎯 EXACT PAPER IMPLEMENTATION - COMPLETE WORKING SCRIPT\")\n",
    "    print(f\"Object: {OBJECT_ID_STR} | Points: {NUM_POINTS} | Batch: {BATCH_SIZE}\")\n",
    "    print(f\"Epochs: {NUM_EPOCHS} | LR: {LEARNING_RATE}\")\n",
    "    print(\"✓ Multi-modal features (RGB + Point Cloud)\")\n",
    "    print(\"✓ Transformer fusion\")\n",
    "    print(\"✓ 6D rotation representation\")\n",
    "    print(\"✓ Per-point confidence voting\")\n",
    "    print(\"✓ PAPER EXACT ARCHITECTURE\\n\")\n",
    "\n",
    "    # Load datasets\n",
    "    train_dataset = SafeDataset(base_dir, OBJECT_ID_STR, is_train=True, num_points=NUM_POINTS)\n",
    "    test_dataset = SafeDataset(base_dir, OBJECT_ID_STR, is_train=False, num_points=NUM_POINTS)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "    print(f\"✓ Training: {len(train_dataset)} samples\")\n",
    "    print(f\"✓ Testing: {len(test_dataset)} samples\")\n",
    "\n",
    "    # Load model info\n",
    "    models_info_file = os.path.join(base_dir, 'models', 'models_info.yml')\n",
    "    with open(models_info_file, 'r') as f:\n",
    "        models_info = yaml.safe_load(f)\n",
    "    object_diameter = models_info[int(OBJECT_ID_STR)]['diameter'] / 1000.0\n",
    "    object_name = OBJECT_NAMES[OBJECT_ID_STR]\n",
    "\n",
    "    print(f\"\\n📊 Object Info:\")\n",
    "    print(f\"  Name: {object_name}\")\n",
    "    print(f\"  Diameter: {object_diameter:.3f}m\")\n",
    "    print(f\"  Symmetric: {object_name in SYMMETRIC_OBJECTS}\")\n",
    "\n",
    "    # Initialize model\n",
    "    model = WorkingPaperModel(\n",
    "        num_points=NUM_POINTS,\n",
    "        feature_dim=FEATURE_DIM,\n",
    "        nhead=NHEAD,\n",
    "        num_layers=NUM_LAYERS\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    # Test forward pass\n",
    "    print(\"\\n🧪 Testing paper forward pass...\")\n",
    "    test_batch = next(iter(train_loader))\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            pred_r, pred_t = model(test_batch['rgb'][:1].to(DEVICE), test_batch['points'][:1].to(DEVICE))\n",
    "        print(\"✅ Paper forward pass successful!\")\n",
    "        print(f\"   Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "        print(f\"   Rotation shape: {pred_r.shape}, Translation shape: {pred_t.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Forward pass failed: {e}\")\n",
    "        raise e\n",
    "\n",
    "    # Training setup\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY, betas=(0.9, 0.999))\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
    "    model_points_tensor = torch.from_numpy(train_dataset.model_points).float().to(DEVICE)\n",
    "\n",
    "    print(f\"\\n🔧 STABILITY IMPROVEMENTS:\")\n",
    "    print(f\"   • Proper weight initialization\")\n",
    "    print(f\"   • Gradient clipping\")\n",
    "    print(f\"   • Learning rate warmup\")\n",
    "    print(f\"   • Batch normalization in MLP\")\n",
    "    print(f\"   • Smaller learning rate (1e-4)\")\n",
    "\n",
    "    # Training\n",
    "    training_history = {'train_loss': [], 'val_metrics': [], 'learning_rates': []}\n",
    "    start_time = time.time()\n",
    "    best_accuracy = 0.0\n",
    "    patience_counter = 0\n",
    "    max_patience = 30\n",
    "\n",
    "    print(f\"\\n🚀 STARTING PAPER TRAINING\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        epoch_start = time.time()\n",
    "\n",
    "        # Train\n",
    "        train_loss = working_train_epoch(model, train_loader, optimizer, model_points_tensor, DEVICE, object_name, epoch)\n",
    "        scheduler.step()\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        # Evaluate every 5 epochs\n",
    "        if epoch % 5 == 0 or epoch == NUM_EPOCHS - 1:\n",
    "            metrics, errors = comprehensive_evaluation(model, test_loader, model_points_tensor, DEVICE, object_name)\n",
    "\n",
    "            training_history['train_loss'].append(float(train_loss))\n",
    "            training_history['val_metrics'].append(metrics)\n",
    "            training_history['learning_rates'].append(float(current_lr))\n",
    "\n",
    "            current_accuracy = metrics['ACC-5cm']\n",
    "\n",
    "            print(f\"\\n📈 Epoch {epoch+1:02d}/{NUM_EPOCHS} - Results:\")\n",
    "            print(f\"   Train Loss: {train_loss:.4f} | LR: {current_lr:.2e}\")\n",
    "            print(f\"   ADD(S) Mean: {metrics['ADD(S)-Mean']:.4f}m\")\n",
    "            print(f\"   Rotation Error: {metrics['Rotation-Error-Mean']:.2f}°\")\n",
    "            print(f\"   Translation Error: {metrics['Translation-Error-Mean']:.4f}m\")\n",
    "            print(f\"   Accuracy @5cm: {metrics['ACC-5cm']:.2f}%\")\n",
    "            print(f\"   Accuracy @10cm: {metrics['ACC-10cm']:.2f}%\")\n",
    "            print(f\"   AUC: {metrics['AUC']:.2f}%\")\n",
    "\n",
    "            if current_accuracy > best_accuracy:\n",
    "                best_accuracy = current_accuracy\n",
    "                patience_counter = 0\n",
    "                torch.save(model.state_dict(), os.path.join(project_dir, 'working_paper_model.pth'))\n",
    "                print(f\"   🎯 NEW BEST! Accuracy: {best_accuracy:.2f}%\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                print(f\"   No improvement ({patience_counter}/{max_patience})\")\n",
    "\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        total_time = time.time() - start_time\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"   ⏱️  Epoch Time: {epoch_time/60:.1f}min | Total: {total_time/60:.1f}min\")\n",
    "\n",
    "        # Early stopping\n",
    "        if patience_counter >= max_patience:\n",
    "            print(f\"🛑 No improvement for {max_patience} epochs - stopping\")\n",
    "            break\n",
    "\n",
    "    # Final evaluation\n",
    "    print(f\"\\n🔍 FINAL EVALUATION\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    final_metrics, final_errors = comprehensive_evaluation(model, test_loader, model_points_tensor, DEVICE, object_name)\n",
    "\n",
    "    print(f\"\\n🏆 PAPER RESULTS - {object_name.upper()}\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Best 5cm Accuracy: {best_accuracy:.2f}%\")\n",
    "    print(f\"Final 5cm Accuracy: {final_metrics['ACC-5cm']:.2f}%\")\n",
    "    print(f\"Final 10cm Accuracy: {final_metrics['ACC-10cm']:.2f}%\")\n",
    "    print(f\"Final AUC: {final_metrics['AUC']:.2f}%\")\n",
    "    print(f\"Final ADD(S) Mean: {final_metrics['ADD(S)-Mean']:.4f}m\")\n",
    "    print(f\"Rotation Error: {final_metrics['Rotation-Error-Mean']:.2f}°\")\n",
    "    print(f\"Translation Error: {final_metrics['Translation-Error-Mean']:.4f}m\")\n",
    "    print(f\"Total Training Time: {total_time/60:.1f} minutes\")\n",
    "    print(f\"Final Epoch: {epoch+1}/{NUM_EPOCHS}\")\n",
    "\n",
    "    # Save results\n",
    "    history_path = os.path.join(project_dir, 'paper_training_history.json')\n",
    "    with open(history_path, 'w') as f:\n",
    "        json.dump(training_history, f, indent=2, default=str)\n",
    "\n",
    "    print(f\"\\n✅ PAPER IMPLEMENTATION COMPLETED!\")\n",
    "    print(f\"   Architecture: EXACT PAPER\")\n",
    "    print(f\"   Model saved: working_paper_model.pth\")\n",
    "    print(f\"   Results saved: paper_training_history.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F5dVeWIN4Hqg"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPLG0Klwlk1n971qJJcGJgH",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
