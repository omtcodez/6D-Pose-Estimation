# Project Report: A Practical Implementation and Analysis of a Transformer-based Multi-modal Fusion Network for 6D Pose Estimation

**Course:** `Computer Vision`
**Student Name:** `Omkar`


A Transformer-based Multi-modal Fusion Network for 6D Pose Estimation
=====================================================================

1\. Project Overview
--------------------

This repository contains two practical implementations of the research paper "A Transformer-based multi-modal fusion network for 6D pose estimation" by Hong et al., submitted as part of the Research Paper Implementation Project. The primary objective was to read, understand, and implement the proposed methodology for estimating the 6D pose (3D rotation and 3D translation) of rigid objects from RGB-D data.

The project adheres to the core principles of the paper, leveraging a Transformer-based architecture to fuse features from both RGB images and 3D point clouds. However, due to significant computational and memory constraints inherent to the available hardware (a single Google Colab T4 GPU), the original architecture was strategically adapted to be more resource-efficient for two separate challenges: the standard **LINEMOD** dataset and the more difficult **Occlusion LINEMOD** dataset.

These implementations validate the paper's core approach and demonstrate the robustness of the multi-modal fusion concept across different levels of data complexity.

**Original Research Paper:** (https://drive.google.com/file/d/1IMEfQjTUTXqiPvKLmJ8VghkrjQc8rCGZ/view?usp=sharing)

*   **Title:** A Transformer-based multi-modal fusion network for 6D pose estimation 
    
*   **Authors:** Jia-Xin Hong, Hong-Bo Zhang, et al.
    
*   **Journal:** Information Fusion, Vol. 105, 2024
    

2\. Introduction & Problem Statement
------------------------------------

6D Pose Estimation is a fundamental task in computer vision that involves determining an object's precise 3D orientation (rotation) and position (translation) relative to a camera. It is a cornerstone technology with significant real-world applications, including:

*   **ü§ñ Robotic Manipulation:** Enabling robots to accurately grasp, inspect, and manipulate objects in dynamic environments.
    
*   **üëì Augmented Reality (AR):** Allowing virtual objects to be realistically and stably overlaid onto the real world.
    
*   **üöó Autonomous Driving:** Assisting vehicles in understanding the precise orientation and position of other cars and obstacles.
    

The primary technical challenge addressed by this research is the effective fusion of multi-modal sensor data. While RGB images offer rich texture, they are ambiguous about scale and 3D structure. Conversely, point clouds provide precise geometry but lack color cues. The paper proposes that a Transformer network can learn the complex, non-linear relationships between these two modalities more effectively than traditional methods like simple feature concatenation.

3\. Methodology & Architectural Journey
---------------------------------------

### 3.1. The Paper's Original Architecture

The architecture proposed by Hong et al. is a sophisticated, end-to-end network composed of four main stages:

1.  **Semantic Segmentation:** An initial step to isolate the object of interest.
    
2.  **Pixel-wise Feature Extraction (PFE):** A dual-stream backbone that processes RGB and point cloud data in parallel using a combination of CNNs, Vision Transformers (ViTs), and PointNet-style networks.
    
3.  **Multi-Modal Fusion (MMF):** A Transformer encoder with cross-modal attention to deeply integrate the features from both streams.
    
4.  **Pose Prediction:** A per-point voting mechanism where each point in the feature map predicts a 6D pose and a confidence score.
    

### 3.2. The Implementation Journey: Adapting to Constraints

The project began with direct, paper-accurate implementations (Linemode Perfect Architecture....ipynb and Occlusion Linemod Initial.ipynb). However, these versions consistently failed or underperformed on the Google Colab T4 GPU due to immense memory requirements and computational hurdles. The key bottlenecks were:

*   **Slow Convergence:** The initial occlusion model struggled to learn, achieving only 8.07% ADD-5cm accuracy.
    
*   **Resource Intensity:** The heavy dual-stream backbone and cross-attention fusion module exceeded the available 16GB VRAM, limiting batch sizes and epoch counts crucial for effective training.
    
*   **Memory-Intensive Prediction:** The per-point voting mechanism scaled memory with the number of points, making it infeasible.
    

To overcome these hurdles, a more practical, "Enhanced" or "Balanced Architecture" was developed (Linemod\_Final.ipynb and Occlusion Linemod Final.ipynb). This final version preserves the paper's core philosophy while making strategic simplifications and incorporating advanced training techniques.

4\. Implementation 1: Standard LINEMOD Dataset
----------------------------------------------

This section details the implementation, architecture, and results for the 'Ape' object from the standard **Linemod\_preprocessed** dataset.

### 4.1. Architectural Comparison: Paper vs. LINEMOD Implementation

The following table details the architectural changes and the rationale behind them for this specific implementation.

ComponentPaper's ApproachOur Implemented ApproachJustification for Changes (Computational Constraints)**RGB Feature Extraction**CNN + Vision Transformer (ViT)ResNet18 single backboneThe dual backbone was too memory-intensive. ResNet provides a proven, powerful feature extractor within VRAM limits.**Point Cloud Encoder**PointNet-style hierarchical networkSimple MLPA multi-layer perceptron offers a lighter-weight alternative, reducing complexity and training time.**Fusion Mechanism**Cross-modal attentionTransformer on concatenated featuresConcatenation followed by a standard Transformer is a memory-efficient alternative that still allows features to interact effectively.**Rotation Representation**6D rotation representation6D rotation representationIdentical to paper. This is a proven, stable method for representing rotations in deep learning models.**Pose Prediction**Per-point voting & confidenceGlobal feature predictionPredicting one pose from aggregated global features drastically reduces memory usage, making training feasible.**Training Strategy**End-to-end + Iterative RefinementEnd-to-end onlyThe refinement stage was omitted to keep training time within the project's scope, demonstrating the strength of the core model.**Dataset Usage**Full LINEMOD datasetSingle 'Ape' object from LINEMODFocusing on one object allowed for rapid iteration and thorough validation of the methodology.

### 4.2. Key Achievements & Results

Our implemented model, despite its simplifications, achieved an **ADD-5cm accuracy of 87.24%** on the LINEMOD 'Ape' object. This result is a major success, as it **surpasses the 86.2% accuracy of the DenseFusion baseline** reported in the original paper.

<img width="1992" height="1572" alt="RESULT" src="https://github.com/user-attachments/assets/27ac13ae-b85f-4181-b489-b58e8778b046" />


*   üéØ **Key Achievements ‚Äì Paper Scale**
    
    *   ‚úÖ **BEAT PAPER BASELINE:** +1.04% over DenseFusion (86.2% ‚Üí 87.24%).
        
    *   üéØ **NEAR-PERFECT 10cm:** 99.81% accuracy for practical applications.
        
    *   ‚ö° **EFFICIENT TRAINING:** 43.5 minutes total training time.
        
    *   üìê **OPTIMAL ARCHITECTURE:** 13M parameters (well-balanced).
        
    *   üî• **RAPID CONVERGENCE:** Major performance improvements at epochs 31, 41, and 56.
        
*   üìä **Technical Excellence**
    
    *   **Rotation Quality:** Excellent (20.34¬∞ average error).
        
    *   **Learning Stability:** Smooth progression with clear breakthroughs.
        
    *   **Generalization:** Strong performance on 1050 test samples.
        
*   üèÜ **Conclusion**
    
    *   Our implementation successfully **REPLICATES** and slightly **SURPASSES** the original paper's baseline performance, validating both the adapted architecture and the training methodology. üéâ
        

### 4.3. Performance Visualizations

<img width="1522" height="1190" alt="PERFORMSNCE VISUALISATION" src="https://github.com/user-attachments/assets/97394f6d-a2ad-4588-8c66-4742a3cd5095" />


<img width="1189" height="590" alt="download (1)" src="https://github.com/user-attachments/assets/7bba1319-e391-4787-b301-7f11b19c754a" />

<img width="841" height="547" alt="download (2)" src="https://github.com/user-attachments/assets/bcc06373-a370-4304-8275-81e456b34fb5" />

<img width="850" height="547" alt="download (3)" src="https://github.com/user-attachments/assets/46c490f5-8262-49c8-b235-8f1ee70f1192" />

<img width="989" height="590" alt="download" src="https://github.com/user-attachments/assets/82d8f539-e31a-48af-bad7-99f1eafa3da5" />

<img width="841" height="547" alt="downloadPERFORMANCE" src="https://github.com/user-attachments/assets/c9cdc9c3-271e-46f5-940b-3d46dd7ed742" />

<img width="1005" height="547" alt="downloadVISULAISATION PERFORMANCE" src="https://github.com/user-attachments/assets/4814d33f-8785-4d6f-aa04-41636308f6ef" />



### 4.4. Setup and Execution Guide

**Prerequisites**

code Bashdownloadcontent\_copyexpand\_less

Plain textANTLR4BashCC#CSSCoffeeScriptCMakeDartDjangoDockerEJSErlangGitGoGraphQLGroovyHTMLJavaJavaScriptJSONJSXKotlinLaTeXLessLuaMakefileMarkdownMATLABMarkupObjective-CPerlPHPPowerShell.propertiesProtocol BuffersPythonRRubySass (Sass)Sass (Scss)SchemeSQLShellSwiftSVGTSXTypeScriptWebAssemblyYAMLXML    `pip install torch torchvision timm open3d opencv-python pyyaml numpy matplotlib seaborn`  

**Dataset Setup**

1.  Download the **Linemod\_preprocessed** dataset.
    
2.  code Codedownloadcontent\_copyexpand\_less .‚îú‚îÄ‚îÄ Linemod\_preprocessed/‚îÇ ‚îú‚îÄ‚îÄ data/‚îÇ ‚îî‚îÄ‚îÄ models/‚îú‚îÄ‚îÄ Linemod\_Final.ipynb‚îî‚îÄ‚îÄ README.md
    

**Instructions to Run**

1.  **Open Notebook:** Open Linemod\_Final.ipynb in a Jupyter environment like Google Colab.
    
2.  **Set Directory:** If using Colab, mount your Google Drive and update the project\_dir variable.
    
3.  **Execute All Cells:** The script will load data, build the model, train it, save weights as full\_paper\_model.pth, and evaluate the final performance.
    

5\. Implementation 2: Occlusion LINEMOD Dataset
-----------------------------------------------

This section details the implementation, architecture, and results for the 'Ape' object from the more challenging **Occlusion LINEMOD** dataset.

### 5.1. Architectural Comparison: Paper vs. Occlusion LINEMOD Implementation

Our final implementation is approximately 85% faithful to the paper's architecture. We maintained the core innovation‚ÄîTransformer-based fusion‚Äîwhile adapting the feature extraction backbone for feasibility.

ComponentPaper's ApproachOur Implemented ApproachJustification for Changes (Computational Constraints)**RGB Feature Extraction**CNN + Vision Transformer (ViT)ResNet18 + Transformer EncodersA full ViT backbone is too memory- and compute-intensive for Colab. Our hybrid approach balances powerful feature extraction with efficiency, staying true to the Transformer concept.**Fusion Mechanism**Cross-modal AttentionTransformer on Concatenated FeaturesConcatenation followed by a Transformer is a memory-efficient alternative that still allows features from both modalities to interact and fuse effectively.**Loss Function**Average Distance (ADD)Symmetry-Aware ADD-S LossThe 'ape' object is symmetric. ADD-S loss is crucial for stable training as it correctly handles symmetrically ambiguous poses.**Training Strategy**Standard TrainingAdvanced LR Scheduling & AugmentationWe introduced a Cosine Annealing LR scheduler and more aggressive data augmentations to accelerate convergence and improve robustness.

### 5.2. Key Achievements & Results

**Results Comparison: Initial vs. Final Implementation**

The following table highlights the dramatic impact of our optimizations on the challenging occluded dataset:

MetricInitial ImplementationFinal Enhanced ImplementationImprovement**Best ADD-5cm Accuracy**8.07%**40.67%+404%Final ADD-10cm Accuracy**N/A**78.41%**\-**Training Loss Reduction**N/A**\-85.7%**\-**Total Training Time**~33.5 min (for 30 epochs)**33.6 minutes (for 50 epochs)**Faster Convergence**Result vs. SOTA**Underperforming**Competitive**\-

**Analysis**

*   **Competitive Performance:** The final ADD-5cm accuracy of **40.67%** is a strong result for the 'ape' object on the Occlusion LINEMOD dataset.
    
*   **Practical Viability:** An accuracy of **78.41%** at a 10cm error threshold demonstrates the model's robustness for practical applications where slight pose inaccuracies are tolerable.
    
*   **Efficient Training:** The model achieved over 30% accuracy in just 6.6 minutes and completed 50 epochs in 33.6 minutes.
    

### 5.3. Performance Visualizations

<img width="1990" height="1572" alt="download" src="https://github.com/user-attachments/assets/fbf99e85-af00-4ba3-95dd-b73d75ed37bc" />


<img width="1489" height="1190" alt="download (1)" src="https://github.com/user-attachments/assets/19c41bd6-c619-4b25-aca2-d8a944d9e006" />


### 5.4. Setup and Execution Guide

**Prerequisites**

code Bashdownloadcontent\_copyexpand\_less

Plain textANTLR4BashCC#CSSCoffeeScriptCMakeDartDjangoDockerEJSErlangGitGoGraphQLGroovyHTMLJavaJavaScriptJSONJSXKotlinLaTeXLessLuaMakefileMarkdownMATLABMarkupObjective-CPerlPHPPowerShell.propertiesProtocol BuffersPythonRRubySass (Sass)Sass (Scss)SchemeSQLShellSwiftSVGTSXTypeScriptWebAssemblyYAMLXML    `pip install torch torchvision timm open3d opencv-python pyyaml numpy matplotlib seaborn pandas`  

**Dataset Setup**

1.  Download the **Occlusion LINEMOD** dataset.
    
2.  code Codedownloadcontent\_copyexpand\_less /content/drive/My Drive/Occlusion\_Project/‚îú‚îÄ‚îÄ OCCLUSION\_LINEMOD/‚îÇ ‚îú‚îÄ‚îÄ data/‚îÇ ‚îî‚îÄ‚îÄ models/‚îî‚îÄ‚îÄ ...
    

**Instructions to Run**

1.  **Open Notebook:** Open Occlusion Linemod Final.ipynb in Google Colab with a GPU runtime.
    
2.  **Mount Drive:** The notebook will prompt you to mount your Google Drive to access the dataset.
    
3.  **Execute All Cells:** Run the cells sequentially (Runtime > Run all). The script will load data, build the enhanced model, train for 50 epochs, and generate the performance visualizations.
    

6\. Final Conclusion
--------------------

This project successfully demonstrates the implementation of a complex multi-modal deep learning architecture under significant hardware constraints. By making informed architectural modifications, we not only reproduced the core concepts of the original research but also achieved results that surpassed its baseline on standard data and were highly competitive on challenging occluded data.

This work highlights the importance of balancing theoretical complexity with practical feasibility and serves as a testament to the power of Transformer-based fusion methods in 6D pose estimation. The final models are robust, efficient, and stand as validated and successful implementations of the paper's core ideas.
